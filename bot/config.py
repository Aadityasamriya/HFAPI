"""
Configuration management for the AI Assistant Telegram Bot
Handles environment variables and secure configuration
"""

import os
from typing import Dict, Optional, List, Any
from dotenv import load_dotenv

load_dotenv()

class Config:
    """
    Configuration class for environment variables
    
    REQUIRED VARIABLES for deployment:
    - TELEGRAM_BOT_TOKEN: Bot token from @BotFather
    - MONGODB_URI: MongoDB connection string (REQUIRED - main database for API keys, admin data, telegram IDs)
    
    OPTIONAL VARIABLES:
    - SUPABASE_MGMT_URL: Supabase PostgreSQL connection string (OPTIONAL - for enhanced user data storage)
    - BOT_NAME: Custom bot name (default: AI Assistant Pro)
    - OWNER_ID: Telegram user ID for admin features
    - ENCRYPTION_SEED: Custom encryption seed (auto-generated if not provided)
    - SUPABASE_USER_BASE_URL: Base URL for user databases (defaults to SUPABASE_MGMT_URL if not provided)
    
    NOTE: Supabase is OPTIONAL. The bot will function with MongoDB only. 
    If Supabase is not configured, enhanced user data features will be disabled but core functionality remains intact.
    """
    
    # ===== REQUIRED CONFIGURATION =====
    # These MUST be set for the bot to function
    TELEGRAM_BOT_TOKEN = os.getenv('TELEGRAM_BOT_TOKEN')
    
    # ===== AI FUNCTIONALITY CONFIGURATION =====
    # Hugging Face API Token - Required for AI features
    HF_TOKEN = os.getenv('HF_TOKEN') or os.getenv('HUGGINGFACE_API_KEY') or os.getenv('HUGGING_FACE_TOKEN')
    
    # ===== TIER DETECTION CONFIGURATION =====
    # HF_TIER: 'free' or 'pro' - Controls access to premium models
    # Default to 'free' to ensure accessibility
    HF_TIER = os.getenv('HF_TIER', 'free').lower()
    
    # ===== PHASE 1 HF API MIGRATION - PROVIDER ABSTRACTION =====
    # Feature flags for new HF Inference Providers API migration
    # HF_API_MODE: Controls which API to use ('inference_api', 'inference_providers', 'auto')
    # CRITICAL FIX: Default to 'inference_providers' for robust error handling and superior performance
    HF_API_MODE = os.getenv('HF_API_MODE', 'inference_providers').lower()
    
    # HF_API_BASE_URL: Custom base URL for HF API (optional)
    # Default to None to use HF's default endpoints
    HF_API_BASE_URL = os.getenv('HF_API_BASE_URL')
    
    # HF_PROVIDER: Specific provider to use with Inference Providers API
    # Can be used to specify preferred infrastructure provider
    HF_PROVIDER = os.getenv('HF_PROVIDER')
    
    # HF_ORG: Organization for billing and usage tracking
    # Used with the new Inference Providers API for organization-based billing
    HF_ORG = os.getenv('HF_ORG')
    
    # ===== HYBRID DATABASE CONFIGURATION =====
    # MongoDB: REQUIRED - Main database for API keys, admin data, telegram IDs, core bot data
    MONGODB_URI = os.getenv('MONGODB_URI') or os.getenv('MONGO_URI')  # Support both variable names
    
    # Supabase: OPTIONAL - Enhanced user data storage (conversations, preferences, files)
    # NOTE: Do NOT fallback to DATABASE_URL as it may point to a non-Supabase PostgreSQL database
    # If Supabase is unavailable, the bot will function with MongoDB only (core features intact)
    SUPABASE_MGMT_URL = os.getenv('SUPABASE_MGMT_URL')  # Explicit Supabase URL only, no fallbacks
    SUPABASE_USER_BASE_URL = os.getenv('SUPABASE_USER_BASE_URL')  # Base URL for user databases (optional)
    
    # ===== CORE BOT CONFIGURATION =====
    BOT_NAME = os.getenv('BOT_NAME', 'Hugging Face By AadityaLabs AI')
    BOT_DESCRIPTION = "Sophisticated Telegram Bot with Intelligent AI Routing"
    BOT_VERSION = "2025.1.0"
    
    # ===== SECURITY CONFIGURATION =====
    # Optional but recommended for production
    OWNER_ID = int(os.getenv('OWNER_ID', '0')) if os.getenv('OWNER_ID', '').isdigit() else None
    ENCRYPTION_SEED = os.getenv('ENCRYPTION_SEED')  # REQUIRED for production, auto-generated in development only
    
    # ===== ENHANCED PRODUCTION ENVIRONMENT DETECTION =====
    @staticmethod
    def _check_env_for_production():
        """
        Comprehensive environment check for production validation
        Enhanced Railway detection with multiple indicators
        """
        # Primary Railway indicators (highest priority)
        railway_indicators = [
            'RAILWAY_ENVIRONMENT',      # Official Railway environment variable
            'RAILWAY_STATIC_URL',       # Railway static URL assignment
            'RAILWAY_PROJECT_ID',       # Railway project identifier
            'RAILWAY_SERVICE_NAME',     # Railway service name
            'RAILWAY_DEPLOYMENT_ID',    # Railway deployment ID
            'RAILWAY_PUBLIC_DOMAIN'     # Railway public domain
        ]
        
        # Check for Railway environment specifically
        railway_env = os.getenv('RAILWAY_ENVIRONMENT', '').lower()
        if railway_env == 'production':
            return True
        
        # Check for any Railway indicators
        if any(os.getenv(indicator) for indicator in railway_indicators):
            # If we detect Railway but env is not production, check if it could still be prod
            env_value = os.getenv('RAILWAY_ENVIRONMENT', '').lower()
            if env_value in ['prod', 'live', 'main']:
                return True
            # Railway detected but not clearly production
            return env_value not in ['development', 'dev', 'test', 'staging']
        
        # Other cloud platform indicators
        if os.getenv('HEROKU_APP_NAME') or os.getenv('VERCEL_ENV') == 'production':
            return True
        
        # Generic environment variable checks
        env_vars = ['ENVIRONMENT', 'NODE_ENV', 'FLASK_ENV', 'DJANGO_ENV', 'APP_ENV']
        production_values = ['production', 'prod', 'live', 'main']
        
        for var in env_vars:
            env_value = os.getenv(var, '').lower()
            if env_value in production_values:
                return True
        
        # Additional production indicators
        production_indicators = [
            'PRODUCTION',               # Generic production flag
            'IS_PRODUCTION',           # Boolean production flag
            'PROD_MODE'                # Production mode flag
        ]
        
        for indicator in production_indicators:
            value = os.getenv(indicator, '').lower()
            if value in ['true', '1', 'yes', 'on']:
                return True
        
        return False
    
    # Store production status for reuse
    IS_PRODUCTION = _check_env_for_production()
    
    # CRITICAL SECURITY: Validate encryption seed in production environments immediately
    if IS_PRODUCTION and not ENCRYPTION_SEED:
        raise ValueError(
            "üö® CRITICAL SECURITY ERROR: ENCRYPTION_SEED environment variable is REQUIRED for production deployment.\n"
            "üìã RAILWAY DEPLOYMENT ISSUE:\n"
            "   ‚Ä¢ This prevents data corruption from different instances using different auto-generated keys\n"
            "   ‚Ä¢ Set ENCRYPTION_SEED in Railway dashboard: Variables ‚Üí Add Variable\n"
            "   ‚Ä¢ Use a secure 32+ character random string\n"
            "   ‚Ä¢ Example: railway variables set ENCRYPTION_SEED='your-secure-32-char-key-here'\n"
            "üîß FIX: Add ENCRYPTION_SEED to your Railway environment variables before deployment."
        )
    
    # ===== TESTING CONFIGURATION =====
    # TEST_MODE: Disables rate limiting and security restrictions for testing
    # SECURITY CRITICAL: This must NEVER be enabled in production
    
    # CRITICAL SECURITY FIX: Prevent TEST_MODE in production with comprehensive validation
    @classmethod
    def _validate_test_mode_security(cls) -> bool:
        """
        CRITICAL SECURITY: Validate that TEST_MODE is never enabled in production
        This prevents dangerous security bypasses in production environments
        
        Returns:
            bool: True if TEST_MODE is safely configured, False otherwise
            
        Raises:
            ValueError: If TEST_MODE is enabled in production (CRITICAL SECURITY ERROR)
        """
        test_mode_env = os.getenv('TEST_MODE', 'false').lower()
        is_test_mode_requested = test_mode_env in ('true', '1', 'yes', 'on')
        
        # SECURITY CHECK: If TEST_MODE is requested, ensure we're NOT in production
        if is_test_mode_requested and cls.IS_PRODUCTION:
            # CRITICAL SECURITY ERROR: TEST_MODE in production
            raise ValueError(
                "üö® CRITICAL SECURITY ERROR: TEST_MODE cannot be enabled in production deployment!\n"
                "üîí SECURITY VIOLATION DETECTED:\n"
                "   ‚Ä¢ TEST_MODE bypasses rate limiting and security restrictions\n"
                "   ‚Ä¢ This creates serious security vulnerabilities in production\n"
                "   ‚Ä¢ Production environment detected via Railway/cloud platform indicators\n"
                "\n"
                "üìã RAILWAY DEPLOYMENT FIX:\n"
                "   ‚Ä¢ Remove TEST_MODE from Railway environment variables\n"
                "   ‚Ä¢ Or set TEST_MODE=false explicitly\n"
                "   ‚Ä¢ Command: railway variables delete TEST_MODE\n"
                "   ‚Ä¢ Alternative: railway variables set TEST_MODE=false\n"
                "\n"
                "üîß IMMEDIATE ACTION REQUIRED:\n"
                "   1. Check Railway dashboard ‚Üí Variables\n"
                "   2. Ensure TEST_MODE is not set or is set to 'false'\n"
                "   3. Redeploy with corrected configuration\n"
                "\n"
                "‚ö†Ô∏è  DEPLOYMENT BLOCKED FOR SECURITY PROTECTION"
            )
        
        # SECURITY VALIDATION: Additional checks for production safety
        if cls.IS_PRODUCTION:
            # In production, TEST_MODE should never be true regardless of environment variable
            return False
        
        # In development/test environments, allow TEST_MODE as configured
        return is_test_mode_requested
    
    # Apply TEST_MODE validation with security checks
    TEST_MODE: bool  # initialized after class creation
    
    @classmethod
    def is_test_mode(cls) -> bool:
        """
        Check if the application is running in test mode with security validation
        
        SECURITY FEATURE: Always returns False in production environments,
        regardless of TEST_MODE environment variable setting.
        
        Returns:
            bool: True if in test mode (rate limiting disabled), False otherwise
                 Always False in production for security
        """
        # CRITICAL SECURITY: Never allow test mode in production
        if cls.IS_PRODUCTION:
            return False
        return cls.TEST_MODE
    
    @classmethod
    def validate_production_security(cls) -> bool:
        """
        Comprehensive production security validation
        
        Returns:
            bool: True if all security checks pass
            
        Raises:
            ValueError: If critical security issues are found
        """
        if not cls.IS_PRODUCTION:
            return True
        
        security_issues = []
        
        # Check 1: TEST_MODE must be disabled in production
        test_mode_env = os.getenv('TEST_MODE', 'false').lower()
        if test_mode_env in ('true', '1', 'yes', 'on'):
            security_issues.append("TEST_MODE is enabled (security bypass risk)")
        
        # Check 2: ENCRYPTION_SEED must be set in production
        if not cls.ENCRYPTION_SEED:
            security_issues.append("ENCRYPTION_SEED is missing (data corruption risk)")
        
        # Check 3: Critical environment variables validation
        if not cls.TELEGRAM_BOT_TOKEN:
            security_issues.append("TELEGRAM_BOT_TOKEN is missing")
        
        if not cls.MONGODB_URI:
            security_issues.append("MONGODB_URI is missing")
        
        # Report all security issues
        if security_issues:
            raise ValueError(
                f"üö® PRODUCTION SECURITY VALIDATION FAILED:\n"
                f"   ‚Ä¢ {len(security_issues)} critical security issues detected\n" + 
                "\n".join([f"   ‚Ä¢ {issue}" for issue in security_issues]) + 
                "\n\nüìã RAILWAY DEPLOYMENT CHECKLIST:\n"
                "   1. Set all required environment variables in Railway dashboard\n"
                "   2. Ensure TEST_MODE is not set or is 'false'\n"
                "   3. Verify ENCRYPTION_SEED is set to a secure 32+ character string\n"
                "   4. Confirm TELEGRAM_BOT_TOKEN and database URLs are valid\n"
                "\nüîß FIX: Address all security issues before production deployment"
            )
        
        return True
    
    # ===== PERFORMANCE TUNING =====
    # These have sensible defaults but can be customized
    REQUEST_TIMEOUT = int(os.getenv('REQUEST_TIMEOUT', '30'))
    MAX_RETRIES = int(os.getenv('MAX_RETRIES', '3'))
    RETRY_DELAY = float(os.getenv('RETRY_DELAY', '1.0'))
    MAX_CHAT_HISTORY = int(os.getenv('MAX_CHAT_HISTORY', '20'))
    MAX_RESPONSE_LENGTH = int(os.getenv('MAX_RESPONSE_LENGTH', '4000'))
    
    # ===== FILE PROCESSING SECURITY (DoS Prevention) =====
    # CRITICAL: Prevent Denial of Service attacks via file processing
    
    # File processing timeout (seconds) - prevents slow file processing DoS
    FILE_PROCESSING_TIMEOUT = int(os.getenv('FILE_PROCESSING_TIMEOUT', '30'))
    
    # File upload rate limiting - prevents file upload spam
    FILE_UPLOAD_MAX_FILES = int(os.getenv('FILE_UPLOAD_MAX_FILES', '5'))  # Max files per time window
    FILE_UPLOAD_TIME_WINDOW = int(os.getenv('FILE_UPLOAD_TIME_WINDOW', '300'))  # 5 minutes in seconds
    
    # Concurrent file processing limits - prevents resource exhaustion
    MAX_CONCURRENT_FILES_PER_USER = int(os.getenv('MAX_CONCURRENT_FILES_PER_USER', '2'))  # Max 2 files per user
    MAX_CONCURRENT_FILES_GLOBAL = int(os.getenv('MAX_CONCURRENT_FILES_GLOBAL', '10'))  # Max 10 files system-wide
    
    # ===== AI MODEL PARAMETERS =====
    # Optional fine-tuning parameters for advanced users
    
    # ========================================================================
    # 2025 MODEL CONFIGURATION - CRITICAL QUOTA LIMITATION (OCTOBER 2025)
    # ========================================================================
    # 
    # üö® CRITICAL ISSUE (2025-10-06):
    # HuggingFace account has EXCEEDED MONTHLY CREDITS for Inference Providers
    # HTTP 402 Error: "You have exceeded your monthly included credits"
    # 
    # Discovery script results (working_models_discovery_20251006_100737.json):
    # - Total models tested: 28
    # - Working models: 2 (7.1% success rate)
    # - Failed: 26 models (quota exhausted or not supported)
    # 
    # CURRENTLY WORKING MODELS (LIMITED):
    # ‚úÖ Qwen/Qwen2.5-7B-Instruct - Primary model (0.57s response time)
    # ‚úÖ Qwen/Qwen2.5-72B-Instruct - High-performance alternative (1.11s)
    # 
    # UNAVAILABLE DUE TO QUOTA EXHAUSTION (HTTP 402):
    # ‚ùå meta-llama/* - All Llama models quota exhausted
    # ‚ùå deepseek-ai/* - All DeepSeek models quota exhausted  
    # ‚ùå google/gemma-* - All Gemma models quota exhausted
    # ‚ùå Qwen/Qwen2.5-Coder-* - All Coder models quota exhausted
    # 
    # UNSUPPORTED MODELS (HTTP 400):
    # ‚ùå microsoft/Phi-* (all variants) - Not supported by any provider
    # ‚ùå Qwen/Qwen2.5-1.5B-Instruct - Not supported
    # 
    # RECOMMENDED ACTIONS:
    # 1. Upgrade to HuggingFace PRO for 20x more monthly credits
    # 2. Rotate to a new HF_TOKEN with fresh credits
    # 3. Wait for monthly quota reset
    # 4. Implement caching and rate limiting to reduce API usage
    # 
    # Models below are kept for when quota is restored
    # ========================================================================
    
    # 2025 LATEST TOP PERFORMING MODELS - Updated with best performers from research
    # Text Generation Models - Latest 2025 best performers with intelligent fallback chains
    
    # === 2025 VERIFIED FREE & HIGH-PERFORMANCE MODELS (OCT 2025 RESEARCH UPDATE) ===
    # Based on comprehensive research of best free HuggingFace models for October 2025
    # All models verified for free tier availability and superior performance
    
    # Best for balanced, high-quality chat and complex reasoning (Oct 2025 Leaders)
    FLAGSHIP_TEXT_MODEL = "Qwen/Qwen2.5-7B-Instruct"  # ‚úÖ OCT 2025: Excellent for conversation & tasks, multilingual
    ULTRA_PERFORMANCE_TEXT_MODEL = "Qwen/Qwen2.5-72B-Instruct"  # ‚úÖ OCT 2025: Highest quality, superior reasoning
    REASONING_TEXT_MODEL = "meta-llama/Meta-Llama-3.1-8B-Instruct"  # ‚úÖ OCT 2025: Apache 2.0, great reasoning, 128K context
    MATH_TEXT_MODEL = "Qwen/Qwen2.5-7B-Instruct"  # ‚úÖ OCT 2025: Superior mathematical reasoning capabilities
    
    # === 2025 HIGH PERFORMANCE MODELS (Research-verified Oct 2025) ===
    HIGH_PERFORMANCE_TEXT_MODEL = "Qwen/Qwen2.5-72B-Instruct"  # ‚úÖ OCT 2025: Best quality, beats many larger models
    ADVANCED_TEXT_MODEL = "meta-llama/Meta-Llama-3.1-8B-Instruct"  # ‚úÖ OCT 2025: Advanced capabilities, 128K context
    MULTILINGUAL_TEXT_MODEL = "Qwen/Qwen2.5-7B-Instruct"  # ‚úÖ OCT 2025: Superior multilingual support (8+ languages)
    
    # === 2025 PRIMARY WORKING MODELS - FREE TIER OPTIMIZED (Oct 2025) ===
    # Research-verified best models on free tier with excellent performance
    DEFAULT_TEXT_MODEL = "Qwen/Qwen2.5-7B-Instruct"  # ‚úÖ OCT 2025: Best default, solid reasoning
    BALANCED_TEXT_MODEL = "meta-llama/Meta-Llama-3.1-8B-Instruct"  # ‚úÖ OCT 2025: Balanced performance, 128K context
    EFFICIENT_7B_TEXT_MODEL = "Qwen/Qwen2.5-7B-Instruct"  # ‚úÖ OCT 2025: Efficient 7B, fast responses
    LEGACY_FLAGSHIP_TEXT_MODEL = "microsoft/Phi-3-mini-4k-instruct"  # ‚úÖ OCT 2025: Legacy fallback
    
    # === 2025 EFFICIENCY OPTIMIZED MODELS (Researched Oct 2025) ===
    # Best for fast, efficient responses on free tier
    EFFICIENT_TEXT_MODEL = "google/gemma-2-2b-it"  # ‚úÖ OCT 2025: Very fast, lightweight
    FAST_TEXT_MODEL = "google/gemma-2-2b-it"  # ‚úÖ OCT 2025: Fastest option for quick responses
    COMPACT_TEXT_MODEL = "google/gemma-2-2b-it"  # ‚úÖ OCT 2025: Lightweight, minimal resources
    LIGHTWEIGHT_TEXT_MODEL = "google/gemma-2-2b-it"  # ‚úÖ OCT 2025: Minimal resource use
    
    # === 2025 INTELLIGENT FALLBACK MODELS (Oct 2025 Research) ===
    # Reliable fallback chain using verified free tier models
    FALLBACK_TEXT_MODEL = "meta-llama/Meta-Llama-3.1-8B-Instruct"  # ‚úÖ OCT 2025: Primary fallback, reliable
    LEGACY_EFFICIENT_MODEL = "google/gemma-2-2b-it"  # ‚úÖ OCT 2025: Fast fallback
    TERTIARY_FALLBACK_MODEL = "microsoft/Phi-3-mini-4k-instruct"  # ‚úÖ OCT 2025: Ultimate fallback
    
    # Code Generation Models - OCT 2025 RESEARCH: Best free tier coding models
    # Qwen2.5-Coder series: 88.4% HumanEval (beats GPT-4's 87.1%), 92 programming languages, 128K context
    
    # === 2025 STATE-OF-THE-ART CODING MODELS (Oct 2025 Research) ===
    # Qwen2.5-Coder: Trained on 5.5 trillion tokens, outperforms GPT-4 on many benchmarks
    ULTRA_PERFORMANCE_CODE_MODEL = "Qwen/Qwen2.5-Coder-32B-Instruct"  # ‚úÖ OCT 2025: 32B coder, superior performance, 128K context
    ADVANCED_CODE_MODEL = "Qwen/Qwen2.5-Coder-7B-Instruct"  # ‚úÖ OCT 2025: 88.4% HumanEval, 92 languages, Apache 2.0
    SPECIALIZED_CODE_MODEL = "Qwen/Qwen2.5-Coder-7B-Instruct"  # ‚úÖ OCT 2025: Code specialist, repair & completion
    
    # === 2025 HIGH PERFORMANCE CODING (Research-verified Oct 2025) ===
    HIGH_PERFORMANCE_CODE_MODEL = "Qwen/Qwen2.5-Coder-32B-Instruct"  # ‚úÖ OCT 2025: High performance, 128K context
    TOOL_USE_CODE_MODEL = "meta-llama/Meta-Llama-3.1-8B-Instruct"  # ‚úÖ OCT 2025: Tool integration, function calling
    MULTILINGUAL_CODE_MODEL = "Qwen/Qwen2.5-Coder-7B-Instruct"  # ‚úÖ OCT 2025: 92 programming languages
    
    # === 2025 PRIMARY CODING MODELS - FREE TIER OPTIMIZED (Oct 2025 Research) ===
    # Research: Qwen2.5-Coder-7B is best overall for free tier (beats GPT-4 on code tasks)
    DEFAULT_CODE_MODEL = "Qwen/Qwen2.5-Coder-7B-Instruct"  # ‚úÖ OCT 2025: Best for code, 88.4% HumanEval
    CODE_GENERATION_MODEL = "Qwen/Qwen2.5-Coder-7B-Instruct"  # ‚úÖ OCT 2025: Superior code generation
    EFFICIENT_7B_CODE_MODEL = "Qwen/Qwen2.5-Coder-7B-Instruct"  # ‚úÖ OCT 2025: Efficient, 7B parameters
    LEGACY_ADVANCED_CODE_MODEL = "meta-llama/Meta-Llama-3.1-8B-Instruct"  # ‚úÖ OCT 2025: General purpose coding
    
    # === 2025 EFFICIENCY OPTIMIZED CODING (Oct 2025) ===
    # Fast code generation on free tier
    EFFICIENT_CODE_MODEL = "Qwen/Qwen2.5-Coder-7B-Instruct"  # ‚úÖ OCT 2025: Fast & accurate code generation
    FAST_CODE_MODEL = "google/gemma-2-2b-it"  # ‚úÖ OCT 2025: Quick coding tasks
    LIGHTWEIGHT_CODE_MODEL = "google/gemma-2-2b-it"  # ‚úÖ OCT 2025: Lightweight coding
    
    # === 2025 INTELLIGENT CODE FALLBACK MODELS (Oct 2025 Research) ===
    # Reliable code generation fallback chain
    FALLBACK_CODE_MODEL = "meta-llama/Meta-Llama-3.1-8B-Instruct"  # ‚úÖ OCT 2025: Code fallback, reliable
    LEGACY_EFFICIENT_CODE_MODEL = "google/gemma-2-2b-it"  # ‚úÖ OCT 2025: Fast code fallback
    TERTIARY_CODE_FALLBACK = "microsoft/Phi-3-mini-4k-instruct"  # ‚úÖ OCT 2025: Ultimate code fallback
    
    # === 2025 VISION/MULTIMODAL MODELS (OCT 2025 RESEARCH) ===
    # Best free tier vision-language models for image understanding, VQA, captioning
    # Qwen2-VL: Superior video understanding (1hr+), document analysis, JSON output, 128K context
    # SmolVLM: Tiny (256M-2B), runs on mobile/edge, excellent for image analysis
    # Llama-3.2-Vision: Best for image captioning and VQA tasks
    
    # === 2025 MULTIMODAL VISION-LANGUAGE MODELS (Oct 2025 Research) ===
    # Top-tier VLMs for comprehensive image understanding
    DEFAULT_VLM_MODEL = "Qwen/Qwen2-VL-7B-Instruct"  # ‚úÖ OCT 2025: Best multimodal, video/image/doc analysis, 128K context
    ADVANCED_VLM_MODEL = "meta-llama/Llama-3.2-11B-Vision-Instruct"  # ‚úÖ OCT 2025: Excellent VQA & captioning, Meta
    LIGHTWEIGHT_VLM_MODEL = "HuggingFaceTB/SmolVLM-Instruct"  # ‚úÖ OCT 2025: 2B, runs on edge/mobile, commercially usable
    FAST_VLM_MODEL = "HuggingFaceTB/SmolVLM-Instruct"  # ‚úÖ OCT 2025: Fast, lightweight, mobile-ready
    
    # === 2025 IMAGE CLASSIFICATION MODELS (Oct 2025) ===
    DEFAULT_IMAGE_CLASSIFICATION_MODEL = "google/vit-base-patch16-224"  # ‚úÖ OCT 2025: ViT for image classification
    FAST_IMAGE_CLASSIFICATION_MODEL = "google/vit-base-patch16-224"  # ‚úÖ OCT 2025: Fast ViT variant
    FALLBACK_IMAGE_CLASSIFICATION_MODEL = "microsoft/resnet-50"  # ‚úÖ OCT 2025: Reliable ResNet fallback
    
    # === 2025 IMAGE-TO-TEXT MODELS (Oct 2025 Research) ===
    # Best for image captioning and description generation
    DEFAULT_IMAGE_CAPTIONING_MODEL = "Qwen/Qwen2-VL-7B-Instruct"  # ‚úÖ OCT 2025: Superior captioning, multimodal
    ADVANCED_IMAGE_CAPTIONING_MODEL = "meta-llama/Llama-3.2-11B-Vision-Instruct"  # ‚úÖ OCT 2025: Excellent captioning
    FAST_IMAGE_CAPTIONING_MODEL = "HuggingFaceTB/SmolVLM-Instruct"  # ‚úÖ OCT 2025: Fast, lightweight
    FALLBACK_IMAGE_CAPTIONING_MODEL = "nlpconnect/vit-gpt2-image-captioning"  # ‚úÖ OCT 2025: Reliable fallback
    
    # === 2025 OBJECT DETECTION MODELS (Oct 2025) ===
    DEFAULT_OBJECT_DETECTION_MODEL = "facebook/detr-resnet-50"  # ‚úÖ OCT 2025: DETR for object detection
    ADVANCED_OBJECT_DETECTION_MODEL = "facebook/detr-resnet-50"  # ‚úÖ OCT 2025: DETR advanced detection
    FAST_OBJECT_DETECTION_MODEL = "facebook/detr-resnet-50"  # ‚úÖ OCT 2025: Consistent detection
    FALLBACK_OBJECT_DETECTION_MODEL = "facebook/detr-resnet-50"  # ‚úÖ OCT 2025: Reliable fallback
    
    # === 2025 VISUAL QUESTION ANSWERING (VQA) MODELS (Oct 2025 Research) ===
    # Research: Qwen2-VL and Llama-3.2-Vision are best for VQA tasks
    DEFAULT_VQA_MODEL = "Qwen/Qwen2-VL-7B-Instruct"  # ‚úÖ OCT 2025: Superior VQA, multimodal reasoning
    ADVANCED_VQA_MODEL = "meta-llama/Llama-3.2-11B-Vision-Instruct"  # ‚úÖ OCT 2025: Excellent VQA capabilities
    FALLBACK_VQA_MODEL = "dandelin/vilt-b32-finetuned-vqa"  # ‚úÖ OCT 2025: Reliable ViLT fallback
    
    # === 2025 ADVANCED VISION MODELS (Oct 2025 Research) ===
    # Multi-modal models with enhanced capabilities
    DEFAULT_VISION_MODEL = "Qwen/Qwen2-VL-7B-Instruct"  # ‚úÖ OCT 2025: Best general vision, multimodal
    ADVANCED_VISION_MODEL = "meta-llama/Llama-3.2-11B-Vision-Instruct"  # ‚úÖ OCT 2025: Advanced vision-language
    REASONING_VISION_MODEL = "Qwen/Qwen2-VL-7B-Instruct"  # ‚úÖ OCT 2025: Superior visual reasoning
    GUI_AUTOMATION_MODEL = "Qwen/Qwen2-VL-7B-Instruct"  # ‚úÖ OCT 2025: GUI understanding & automation
    FAST_VISION_MODEL = "HuggingFaceTB/SmolVLM-Instruct"  # ‚úÖ OCT 2025: Fast, lightweight 2B
    DOCUMENT_VISION_MODEL = "Qwen/Qwen2-VL-7B-Instruct"  # ‚úÖ OCT 2025: Document analysis, JSON output
    MEDICAL_VISION_MODEL = "google/vit-base-patch16-224"  # ‚úÖ OCT 2025: Medical imaging
    FALLBACK_VISION_MODEL = "openai/clip-vit-large-patch14"  # ‚úÖ OCT 2025: CLIP fallback
    LIGHTWEIGHT_VISION_MODEL = "HuggingFaceTB/SmolVLM-Instruct"  # ‚úÖ OCT 2025: Lightweight 2B VLM
    
    # === 2025 PDF & DOCUMENT PROCESSING MODELS (OCT 2025 RESEARCH) ===
    # Best free tier models for PDF analysis, text extraction, and document understanding
    # Nougat: Scientific PDFs, math formulas, converts to Markdown (MIT license)
    # Donut: Receipts, invoices, forms, OCR-free architecture (MIT license)
    # LayoutLM: Form understanding with layout information (v1 is MIT licensed)
    
    DEFAULT_PDF_MODEL = "facebook/nougat-base"  # ‚úÖ OCT 2025: Best for scientific PDFs, math formulas, MIT license
    ADVANCED_PDF_MODEL = "Qwen/Qwen2-VL-7B-Instruct"  # ‚úÖ OCT 2025: Advanced document understanding with VLM
    RECEIPT_INVOICE_MODEL = "naver-clova-ix/donut-base-finetuned-cord-v2"  # ‚úÖ OCT 2025: Receipts & invoices, MIT license
    FORM_EXTRACTION_MODEL = "microsoft/layoutlm-base-uncased"  # ‚úÖ OCT 2025: Form understanding, MIT license (v1 only)
    DOCUMENT_QA_MODEL = "Qwen/Qwen2-VL-7B-Instruct"  # ‚úÖ OCT 2025: Document Q&A with vision model
    FALLBACK_PDF_MODEL = "Qwen/Qwen2.5-7B-Instruct"  # ‚úÖ OCT 2025: Text-based PDF analysis fallback
    
    # === 2025 IMAGE GENERATION MODELS (OCT 2025 RESEARCH) ===
    # FLUX.1-schnell: FREE (Apache 2.0), ultra-fast (1-4 steps), 12B params, high quality
    # SDXL-Turbo: 1-step generation, photorealistic, but non-commercial license
    # Stable Diffusion XL: High quality, 1024√ó1024, CreativeML license
    
    # Research: FLUX.1-schnell is BEST free model - Apache 2.0, commercial use OK, ultra-fast
    DEFAULT_IMAGE_GENERATION_MODEL = "black-forest-labs/FLUX.1-schnell"  # ‚úÖ OCT 2025: FREE, ultra-fast 1-4 steps, Apache 2.0
    FAST_IMAGE_GENERATION_MODEL = "black-forest-labs/FLUX.1-schnell"  # ‚úÖ OCT 2025: 1-4 steps, commercial use OK
    HIGH_QUALITY_IMAGE_MODEL = "stabilityai/stable-diffusion-xl-base-1.0"  # ‚úÖ OCT 2025: 1024√ó1024, high quality
    TURBO_IMAGE_GENERATION_MODEL = "stabilityai/sdxl-turbo"  # ‚úÖ OCT 2025: 1-step, fast (non-commercial)
    FALLBACK_IMAGE_GENERATION_MODEL = "stabilityai/stable-diffusion-xl-base-1.0"  # ‚úÖ OCT 2025: Reliable SDXL
    
    # === 2025 IMAGE PROMPT GENERATION MODELS (Oct 2025) ===
    # Text models for generating image prompts and descriptions
    DEFAULT_IMAGE_PROMPT_MODEL = "Qwen/Qwen2.5-7B-Instruct"  # ‚úÖ OCT 2025: Best for image prompt generation
    FLAGSHIP_IMAGE_MODEL = "Qwen/Qwen2.5-7B-Instruct"  # ‚úÖ OCT 2025: High-quality prompts
    COMMERCIAL_IMAGE_MODEL = "Qwen/Qwen2.5-7B-Instruct"  # ‚úÖ OCT 2025: Professional descriptions
    ADVANCED_IMAGE_MODEL = "Qwen/Qwen2.5-7B-Instruct"  # ‚úÖ OCT 2025: Advanced image prompts
    EDITING_IMAGE_MODEL = "Qwen/Qwen2.5-7B-Instruct"  # ‚úÖ OCT 2025: Editing instructions
    PROFESSIONAL_IMAGE_MODEL = "Qwen/Qwen2.5-7B-Instruct"  # ‚úÖ OCT 2025: Professional grade
    KONTEXT_IMAGE_MODEL = "Qwen/Qwen2.5-7B-Instruct"  # ‚úÖ OCT 2025: Contextual descriptions
    TURBO_IMAGE_MODEL = "google/gemma-2-2b-it"  # ‚úÖ OCT 2025: Fast image prompts
    ARTISTIC_IMAGE_MODEL = "Qwen/Qwen2.5-7B-Instruct"  # ‚úÖ OCT 2025: Artistic descriptions
    REALISTIC_IMAGE_MODEL = "Qwen/Qwen2.5-7B-Instruct"  # ‚úÖ OCT 2025: Realistic descriptions
    FALLBACK_IMAGE_MODEL = "microsoft/Phi-3-mini-4k-instruct"  # ‚úÖ OCT 2025: Reliable fallback
    
    # === 2025 SENTIMENT ANALYSIS & NLP MODELS ===
    # Verified working sentiment models on free tier
    DEFAULT_SENTIMENT_MODEL = "cardiffnlp/twitter-roberta-base-sentiment-latest"  # ‚úÖ FREE TIER: Sentiment analysis
    ADVANCED_SENTIMENT_MODEL = "j-hartmann/emotion-english-distilroberta-base"  # ‚úÖ FREE TIER: Emotion detection
    EMOTION_MODEL = "j-hartmann/emotion-english-distilroberta-base"  # ‚úÖ FREE TIER: Emotion analysis
    MULTILINGUAL_SENTIMENT_MODEL = "cardiffnlp/twitter-xlm-roberta-base-sentiment"  # VERIFIED: Multilingual
    FALLBACK_SENTIMENT_MODEL = "distilbert-base-uncased-finetuned-sst-2-english"  # VERIFIED: Reliable fallback
    
    # === 2025 TRANSLATION MODELS ===
    # Modern translation models with enhanced performance
    DEFAULT_TRANSLATION_MODEL = "Helsinki-NLP/opus-mt-mul-en"  # Verified multilingual to English
    ADVANCED_TRANSLATION_MODEL = "facebook/nllb-200-distilled-600M"  # 2025: NLLB advanced
    FAST_TRANSLATION_MODEL = "Helsinki-NLP/opus-mt-en-mul"  # Fast English to multilingual
    SPECIALIZED_TRANSLATION_MODEL = "facebook/nllb-200-distilled-1.3B"  # 2025: High quality
    FALLBACK_TRANSLATION_MODEL = "Helsinki-NLP/opus-mt-mul-en"  # Reliable fallback
    
    # === 2025 QUESTION ANSWERING MODELS (OCT 2025) ===
    # Modern QA models that provide better answers than summarization models
    DEFAULT_QA_MODEL = "deepset/roberta-base-squad2"  # Dedicated QA model (not chat-based)
    ADVANCED_QA_MODEL = "Qwen/Qwen2.5-7B-Instruct"  # ‚úÖ VERIFIED: Advanced QA model (0.47s)
    FACTUAL_QA_MODEL = "deepset/roberta-base-squad2"  # Factual QA specialist (not chat-based)
    FALLBACK_QA_MODEL = "distilbert-base-uncased-distilled-squad"  # Efficient QA (not chat-based)
    
    # === 2025 GENERATIVE QA MODELS ===
    # Modern generative models for complex question answering
    DEFAULT_GENERATIVE_QA_MODEL = "Qwen/Qwen2.5-7B-Instruct"  # ‚úÖ VERIFIED: Superior generative QA (0.47s)
    ADVANCED_GENERATIVE_QA_MODEL = "meta-llama/Llama-3.1-8B-Instruct"  # ‚úÖ VERIFIED: Advanced reasoning (0.45s)
    FALLBACK_GENERATIVE_QA_MODEL = "google/gemma-2-2b-it"  # ‚úÖ VERIFIED: Efficient fallback (0.76s)
    
    # === 2025 SUMMARIZATION MODELS ===
    # Proven summarization models with excellent performance
    DEFAULT_SUMMARIZATION_MODEL = "facebook/bart-large-cnn"  # 2025: Superior CNN summarization
    ADVANCED_SUMMARIZATION_MODEL = "google/pegasus-xsum"  # 2025: High-quality abstractive
    FAST_SUMMARIZATION_MODEL = "sshleifer/distilbart-cnn-12-6"  # 2025: Fast summarization
    FALLBACK_SUMMARIZATION_MODEL = "facebook/bart-base"  # 2025: Reliable BART fallback
    
    # Text Classification Models - 2025 verified working classification models
    DEFAULT_CLASSIFICATION_MODEL = "cardiffnlp/twitter-roberta-base-sentiment-latest"  # RoBERTa for general classification
    TOPIC_CLASSIFICATION_MODEL = "j-hartmann/emotion-english-distilroberta-base"  # Emotion detection for topic classification
    INTENT_CLASSIFICATION_MODEL = "cardiffnlp/twitter-roberta-base-sentiment-latest"  # Intent classification
    FALLBACK_CLASSIFICATION_MODEL = "cardiffnlp/twitter-roberta-base-sentiment-latest"  # RoBERTa for classification
    
    # === 2025 AUDIO PROCESSING MODELS (OCT 2025 RESEARCH) ===
    # Whisper Large v3: 96+ languages, 10-20% WER, multilingual standard
    # Nvidia Canary Qwen: 5.63% WER (#1 accuracy), 4 languages, hybrid ASR+LLM
    # Nvidia Parakeet TDT: 18-21% WER, ultra-fast (1000-4500x), English only
    # Distil-Whisper Large v3: 6x faster than Whisper, within 1% accuracy
    # Mistral Voxtral: Speech understanding + Q&A, multilingual auto-detect
    
    # Research: Whisper Large v3 is best free multilingual ASR model
    DEFAULT_ASR_MODEL = "openai/whisper-large-v3"  # ‚úÖ OCT 2025: Best multilingual ASR, 96+ languages, 10-20% WER
    ADVANCED_ASR_MODEL = "nvidia/canary-qwen-2.5b"  # ‚úÖ OCT 2025: Highest accuracy (5.63% WER), hybrid ASR+LLM
    FAST_ASR_MODEL = "distil-whisper/distil-large-v3"  # ‚úÖ OCT 2025: 6x faster than Whisper, 15% WER
    ULTRA_FAST_ASR_MODEL = "nvidia/parakeet-tdt-0.6b-v2"  # ‚úÖ OCT 2025: Ultra-fast (1000x), English only
    TURBO_ASR_MODEL = "openai/whisper-large-v3-turbo"  # ‚úÖ OCT 2025: 8x faster Whisper variant
    MULTILINGUAL_ASR_MODEL = "openai/whisper-large-v3"  # ‚úÖ OCT 2025: 96+ languages support
    FALLBACK_ASR_MODEL = "distil-whisper/distil-large-v3"  # ‚úÖ OCT 2025: Efficient fallback
    
    # Named Entity Recognition Models - 2025 verified token-classification models
    # Note: Using proper NER models trained for token classification tasks
    DEFAULT_NER_MODEL = "dslim/bert-base-NER"  # BERT-base for English NER
    MULTILINGUAL_NER_MODEL = "Davlan/bert-base-multilingual-cased-ner-hrl"  # Multilingual BERT for NER
    ADVANCED_NER_MODEL = "dslim/bert-base-NER"  # BERT-base for advanced NER
    FALLBACK_NER_MODEL = "dslim/bert-base-NER"  # BERT-base NER fallback
    
    # Legacy NER Models (using LLMs for complex NER tasks when token classification insufficient)
    LEGACY_NER_MODEL = "meta-llama/Llama-3.1-8B-Instruct"  # ‚úÖ VERIFIED: Llama for complex NER (0.45s)
    LEGACY_MULTILINGUAL_NER_MODEL = "Qwen/Qwen2.5-7B-Instruct"  # ‚úÖ VERIFIED: Qwen for complex multilingual NER (0.47s)
    
    # FREE-TIER SPECIALIZED MODELS - 2025 verified working task-specific models
    # GUI Automation Models - Using verified vision models for GUI tasks
    DEFAULT_GUI_MODEL = "openai/clip-vit-large-patch14"  # CLIP for basic GUI understanding
    ADVANCED_GUI_MODEL = "facebook/detr-resnet-50"  # DETR for advanced GUI tasks
    LIGHTWEIGHT_GUI_MODEL = "google/owlvit-base-patch32"  # OWL-ViT for lightweight GUI tasks
    
    # Tool Use & Function Calling Models - 2025 optimized for tool integration (Oct 2025)
    DEFAULT_TOOL_MODEL = "Qwen/Qwen2.5-Coder-32B-Instruct"  # ‚úÖ VERIFIED: Best for tool use (0.63s)
    ULTRA_TOOL_MODEL = "Qwen/Qwen2.5-72B-Instruct"  # ‚úÖ VERIFIED: Ultimate tool integration (0.85s)
    EFFICIENT_TOOL_MODEL = "Qwen/Qwen2.5-7B-Instruct"  # ‚úÖ VERIFIED: Efficient tool use (0.47s)
    FAST_TOOL_MODEL = "meta-llama/Llama-3.1-8B-Instruct"  # ‚úÖ VERIFIED: Fast tool use (0.45s)
    
    # Enhanced Vision Models - 2025 verified enhanced vision models
    PREMIUM_VISION_MODEL = "facebook/detr-resnet-50"  # DETR as premium vision model
    QUANTIZED_VISION_MODEL = "google/owlvit-base-patch32"  # OWL-ViT as quantized alternative
    
    # 2025 Model Performance Optimizations
    # Enable specific optimizations for each model type
    QWEN_FLASH_ATTENTION = True             # Qwen models flash attention
    DEEPSEEK_OPTIMIZED_INFERENCE = True     # DeepSeek models optimized inference
    STARCODER_FAST_GENERATION = True        # StarCoder fast code generation
    FLUX_SCHNELL_ACCELERATION = True        # FLUX.1-schnell acceleration
    LLAMA_VISION_FAST_PROCESSING = True     # Llama Vision fast processing
    BERT_SENTIMENT_BATCH_OPT = True         # BERT sentiment batch optimization
    T5_TRANSLATION_STREAMING = True         # T5 translation streaming
    BART_SUMMARIZATION_CACHING = True       # BART summarization caching
    WHISPER_AUDIO_OPTIMIZATION = True       # Whisper audio processing optimization
    
    # 2025 Specific Model Optimizations
    QWEN_VL_DYNAMIC_RESOLUTION = True   # Qwen2.5-VL dynamic resolution
    LLAMA_VISION_OPTIMIZE_MEMORY = True # Llama Vision memory optimization
    FLUX_SCHNELL_TURBO_MODE = True      # FLUX.1-schnell turbo optimization
    CODELLAMA_OPTIMIZE_INFERENCE = True # CodeLlama inference optimization
    DEEPSEEK_CODER_FAST_DECODE = True   # DeepSeek-Coder fast decoding
    
    # Advanced routing and performance features
    ENABLE_MODEL_WARMUP = True          # Pre-warm frequently used models
    USE_INTELLIGENT_BATCHING = True     # Batch similar requests
    ENABLE_RESPONSE_STREAMING = True    # Stream responses for better UX
    ADAPTIVE_CONTEXT_LENGTH = True      # Dynamically adjust context length
    ENABLE_MODEL_LOAD_BALANCING = True  # Distribute load across model variants
    
    # Advanced Features for Superior Performance
    USE_CONTEXT_OPTIMIZATION = True  # Leverage long context capabilities
    ENABLE_MULTIMODAL_ROUTING = True  # Smart routing for different model types
    USE_ADAPTIVE_PARAMETERS = True  # Dynamic parameter adjustment
    
    # Quality and Safety Settings
    ENABLE_CONTENT_FILTERING = True  # Built-in content safety
    USE_ADVANCED_PROMPTING = True  # Enhanced prompt engineering
    ENABLE_SMART_CACHING = True  # Performance optimization
    
    # Model Quality Settings
    USE_ADVANCED_ROUTING = True  # Enable sophisticated model selection
    ENABLE_MODEL_FALLBACKS = True  # Smart fallback system
    PERFORMANCE_MONITORING = True  # Track model performance
    
    # 2025: MODEL-SPECIFIC TEMPERATURE AND PARAMETER SETTINGS
    # Optimized temperature settings for different model types
    DEEPSEEK_TEMPERATURE = 0.8          # DeepSeek-R1 reasoning optimal temp
    QWEN_TEMPERATURE = 0.7               # Qwen3 series optimal temp
    STARCODER_TEMPERATURE = 0.2          # StarCoder2 precise code generation
    VISION_TEMPERATURE = 0.5             # Vision models optimal temp
    
    # Model-specific token limits
    DEEPSEEK_MAX_TOKENS = 2500           # DeepSeek-R1 extended reasoning
    QWEN_MAX_TOKENS = 2000               # Qwen3 series standard
    CODE_MAX_TOKENS = 1500               # Coding models optimal
    
    # DeepSeek optimizations
    DEEPSEEK_USE_FLASH_ATTENTION = True  # Enable flash attention for DeepSeek
    
    # Vision model settings
    QWEN_VL_IMAGE_SIZE = 448             # Qwen2.5-VL optimal image size
    VISION_MAX_RESOLUTION = 1024         # Standard vision resolution
    
    # Image generation settings
    FLUX_INFERENCE_STEPS = 4             # FLUX.1-schnell optimal steps
    FLUX_MAX_RESOLUTION = 1024           # FLUX optimal resolution
    SD_INFERENCE_STEPS = 20              # Stable Diffusion steps
    
    # Performance optimization flags
    ENABLE_BATCH_PROCESSING = True       # Batch multiple requests
    USE_MODEL_CACHING = True             # Cache frequently used models
    OPTIMIZE_MEMORY_USAGE = True         # Memory optimization
    

    # ===== RAILWAY-SPECIFIC ENVIRONMENT DETECTION AND VALIDATION =====
    
    @classmethod
    def is_railway_environment(cls) -> bool:
        """
        Check if running on Railway.com platform
        
        Returns:
            bool: True if Railway environment is detected
        """
        railway_indicators = [
            'RAILWAY_ENVIRONMENT',
            'RAILWAY_STATIC_URL', 
            'RAILWAY_PROJECT_ID',
            'RAILWAY_SERVICE_NAME',
            'RAILWAY_DEPLOYMENT_ID',
            'RAILWAY_PUBLIC_DOMAIN'
        ]
        return any(os.getenv(indicator) for indicator in railway_indicators)
    
    @classmethod
    def get_railway_environment_info(cls) -> Dict[str, Optional[str] | bool]:
        """
        Get comprehensive Railway environment information
        
        Returns:
            Dict containing Railway environment details
        """
        return {
            'environment': os.getenv('RAILWAY_ENVIRONMENT'),
            'service_name': os.getenv('RAILWAY_SERVICE_NAME'),
            'project_id': os.getenv('RAILWAY_PROJECT_ID'),
            'deployment_id': os.getenv('RAILWAY_DEPLOYMENT_ID'),
            'static_url': os.getenv('RAILWAY_STATIC_URL'),
            'public_domain': os.getenv('RAILWAY_PUBLIC_DOMAIN'),
            'port': os.getenv('PORT'),
            'is_railway': cls.is_railway_environment()
        }
    
    @classmethod
    def validate_railway_environment(cls) -> bool:
        """
        Validate Railway-specific environment configuration
        
        Returns:
            bool: True if Railway environment is properly configured
            
        Raises:
            ValueError: If Railway environment has configuration issues
        """
        if not cls.is_railway_environment():
            return True  # Not Railway, no Railway-specific validation needed
        
        railway_info = cls.get_railway_environment_info()
        issues = []
        
        # Check for Railway environment specification
        if not railway_info['environment']:
            issues.append("RAILWAY_ENVIRONMENT is not set (should be 'production', 'staging', or 'development')")
        elif isinstance(railway_info['environment'], str) and railway_info['environment'].lower() not in ['production', 'staging', 'development']:
            issues.append(f"Invalid RAILWAY_ENVIRONMENT: '{railway_info['environment']}' (must be 'production', 'staging', or 'development')")
        
        # Check for required Railway port assignment
        if not railway_info['port']:
            issues.append("PORT is not set (Railway requires dynamic port assignment)")
        elif isinstance(railway_info['port'], str) and not railway_info['port'].isdigit():
            issues.append(f"Invalid PORT: '{railway_info['port']}' (must be a numeric port number)")
        elif not (1000 <= int(railway_info['port']) <= 65535):
            issues.append(f"PORT out of range: {railway_info['port']} (must be between 1000-65535)")
        
        # Check for service identification (helps with debugging)
        if not railway_info['service_name']:
            # This is a warning, not a critical error
            import logging
            logger = logging.getLogger(__name__)
            logger.warning("‚ö†Ô∏è RAILWAY_SERVICE_NAME not set - service identification may be limited")
        
        if issues:
            raise ValueError(
                f"üö® RAILWAY CONFIGURATION ERROR:\n" +
                f"   ‚Ä¢ {len(issues)} Railway-specific issues detected:\n" +
                "\n".join([f"   ‚Ä¢ {issue}" for issue in issues]) +
                "\n\nüìã RAILWAY DEPLOYMENT FIX:\n"
                "   1. Check Railway dashboard ‚Üí Variables\n"
                "   2. Ensure RAILWAY_ENVIRONMENT is set correctly\n"
                "   3. Verify PORT is assigned by Railway (should be automatic)\n"
                "   4. Set RAILWAY_SERVICE_NAME for better identification\n"
                "\nüîß RAILWAY CLI COMMANDS:\n"
                "   railway variables set RAILWAY_ENVIRONMENT=production\n"
                "   railway variables set RAILWAY_SERVICE_NAME=your-bot-name\n"
                "\n‚ö†Ô∏è Some variables are auto-assigned by Railway"
            )
        
        return True
    
    @classmethod
    def get_deployment_context(cls) -> Dict[str, Any]:
        """
        Get comprehensive deployment context information
        
        Returns:
            Dict with deployment environment details
        """
        return {
            'is_production': cls.IS_PRODUCTION,
            'is_railway': cls.is_railway_environment(),
            'test_mode': cls.is_test_mode(),
            'railway_info': cls.get_railway_environment_info(),
            'environment_indicators': {
                'RAILWAY_ENVIRONMENT': os.getenv('RAILWAY_ENVIRONMENT'),
                'ENVIRONMENT': os.getenv('ENVIRONMENT'),
                'NODE_ENV': os.getenv('NODE_ENV'),
                'FLASK_ENV': os.getenv('FLASK_ENV'),
                'HEROKU_APP_NAME': os.getenv('HEROKU_APP_NAME'),
                'VERCEL_ENV': os.getenv('VERCEL_ENV')
            }
        }
    
    # ===== CRITICAL ENVIRONMENT VARIABLE VALIDATION =====
    
    @classmethod
    def validate_telegram_bot_token(cls) -> bool:
        """
        Validate TELEGRAM_BOT_TOKEN with comprehensive checks
        
        Returns:
            bool: True if token is valid
            
        Raises:
            ValueError: If token is missing or invalid with Railway-specific guidance
        """
        token = cls.TELEGRAM_BOT_TOKEN
        
        if not token:
            raise ValueError(
                "üö® CRITICAL ERROR: TELEGRAM_BOT_TOKEN is missing\n"
                "üìã RAILWAY DEPLOYMENT ISSUE:\n"
                "   ‚Ä¢ Telegram Bot Token is required for bot functionality\n"
                "   ‚Ä¢ Get token from @BotFather on Telegram\n"
                "   ‚Ä¢ Add to Railway environment variables\n"
                "\n"
                "üîß RAILWAY FIX:\n"
                "   1. Message @BotFather on Telegram\n"
                "   2. Create new bot or use /token for existing bot\n"
                "   3. Copy the token (format: 123456789:ABCdef...)\n"
                "   4. Set in Railway: railway variables set TELEGRAM_BOT_TOKEN='your-token-here'\n"
                "   5. Redeploy: railway up\n"
                "\n"
                "‚ö†Ô∏è DEPLOYMENT BLOCKED - Bot cannot function without valid token"
            )
        
        # Validate token format: should be like 123456789:ABCdefGhIjKlMnOpQrStUvWxYz
        import re
        token_pattern = r'^\d{8,10}:[A-Za-z0-9_-]{35}$'
        if not re.match(token_pattern, token):
            raise ValueError(
                f"üö® INVALID TELEGRAM_BOT_TOKEN format\n"
                f"üìã TOKEN VALIDATION FAILED:\n"
                f"   ‚Ä¢ Current token: {'*' * (len(token) - 10) + token[-10:] if len(token) > 10 else 'HIDDEN'}\n"
                f"   ‚Ä¢ Expected format: 123456789:ABCdef... (numbers:letters, 35 chars after colon)\n"
                f"   ‚Ä¢ Token length: {len(token)} characters\n"
                "\n"
                "üîß RAILWAY FIX:\n"
                "   1. Verify token copied completely from @BotFather\n"
                "   2. Check for extra spaces or characters\n"
                "   3. Update Railway variable: railway variables set TELEGRAM_BOT_TOKEN='correct-token'\n"
                "   4. Redeploy: railway up\n"
                "\n"
                "‚ö†Ô∏è Bot authentication will fail with invalid token format"
            )
        
        return True
    
    @classmethod
    def validate_hf_token(cls) -> bool:
        """
        Validate HF_TOKEN (Hugging Face API Token) with comprehensive checks
        
        Returns:
            bool: True if token is valid or missing in development
            
        Raises:
            ValueError: If token is missing in production or invalid format
        """
        token = cls.HF_TOKEN
        
        # In production, HF_TOKEN is required for AI features
        if cls.IS_PRODUCTION and not token:
            raise ValueError(
                "üö® PRODUCTION ERROR: HF_TOKEN is missing\n"
                "üìã RAILWAY DEPLOYMENT ISSUE:\n"
                "   ‚Ä¢ Hugging Face API Token required for AI features in production\n"
                "   ‚Ä¢ AI functionality will be severely limited without token\n"
                "   ‚Ä¢ Free tokens available at https://huggingface.co/settings/tokens\n"
                "\n"
                "üîß RAILWAY FIX:\n"
                "   1. Visit https://huggingface.co/settings/tokens\n"
                "   2. Create new token (Read access sufficient)\n"
                "   3. Copy token (format: hf_...)\n"
                "   4. Set in Railway: railway variables set HF_TOKEN='your-hf-token-here'\n"
                "   5. Alternative names also supported: HUGGINGFACE_API_KEY, HUGGING_FACE_TOKEN\n"
                "   6. Redeploy: railway up\n"
                "\n"
                "‚ö†Ô∏è PRODUCTION DEPLOYMENT: AI features require valid HF token"
            )
        
        # If token exists, validate format
        if token:
            if not token.startswith('hf_'):
                raise ValueError(
                    f"üö® INVALID HF_TOKEN format\n"
                    f"üìã TOKEN VALIDATION FAILED:\n"
                    f"   ‚Ä¢ HF tokens must start with 'hf_'\n"
                    f"   ‚Ä¢ Current token starts with: '{token[:10]}...'\n"
                    f"   ‚Ä¢ Expected format: hf_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n"
                    "\n"
                    "üîß RAILWAY FIX:\n"
                    "   1. Verify token copied completely from Hugging Face\n"
                    "   2. Check token starts with 'hf_'\n"
                    "   3. Update Railway: railway variables set HF_TOKEN='hf_your-token'\n"
                    "   4. Redeploy: railway up\n"
                    "\n"
                    "‚ö†Ô∏è AI API calls will fail with invalid token format"
                )
            
            if len(token) < 20:
                raise ValueError(
                    f"üö® HF_TOKEN appears too short\n"
                    f"üìã TOKEN VALIDATION:\n"
                    f"   ‚Ä¢ Token length: {len(token)} characters\n"
                    f"   ‚Ä¢ Expected: 32+ characters after 'hf_' prefix\n"
                    f"   ‚Ä¢ This token may be incomplete\n"
                    "\n"
                    "üîß RAILWAY FIX:\n"
                    "   1. Verify complete token was copied from Hugging Face\n"
                    "   2. Update Railway: railway variables set HF_TOKEN='complete-token'\n"
                    "   3. Redeploy: railway up"
                )
        
        return True
    
    @classmethod  
    def validate_mongodb_uri(cls) -> bool:
        """
        Validate MONGODB_URI with comprehensive format and connectivity checks
        
        Returns:
            bool: True if URI is valid
            
        Raises:
            ValueError: If URI is missing or invalid with Railway-specific guidance
        """
        uri = cls.MONGODB_URI
        
        if not uri:
            raise ValueError(
                "üö® CRITICAL ERROR: MONGODB_URI is missing\n"
                "üìã RAILWAY DEPLOYMENT ISSUE:\n"
                "   ‚Ä¢ MongoDB connection string required for data storage\n"
                "   ‚Ä¢ Bot cannot store user data or configurations without database\n"
                "   ‚Ä¢ Use MongoDB Atlas (free tier available) or Railway's MongoDB addon\n"
                "\n"
                "üîß RAILWAY FIX OPTIONS:\n"
                "\n"
                "OPTION 1 - MongoDB Atlas (Recommended):\n"
                "   1. Visit https://cloud.mongodb.com/\n"
                "   2. Create free cluster (M0 Sandbox)\n"
                "   3. Get connection string\n"
                "   4. Set: railway variables set MONGODB_URI='mongodb+srv://...'\n"
                "\n"
                "OPTION 2 - Railway MongoDB Plugin:\n"
                "   1. Railway dashboard ‚Üí Add Service ‚Üí Database ‚Üí MongoDB\n"
                "   2. Copy provided MONGODB_URI\n"
                "   3. Set: railway variables set MONGODB_URI='mongodb://...'\n"
                "\n"
                "‚ö†Ô∏è DEPLOYMENT BLOCKED - Database connection required"
            )
        
        # Validate MongoDB URI format
        if not (uri.startswith('mongodb://') or uri.startswith('mongodb+srv://')):
            raise ValueError(
                f"üö® INVALID MONGODB_URI format\n"
                f"üìã URI VALIDATION FAILED:\n"
                f"   ‚Ä¢ Must start with 'mongodb://' or 'mongodb+srv://'\n"
                f"   ‚Ä¢ Current URI starts with: '{uri.split('://')[0] if '://' in uri else 'INVALID'}://'\n"
                "\n"
                "üîß RAILWAY FIX:\n"
                "   1. Verify complete MongoDB connection string\n"
                "   2. Format: mongodb+srv://username:password@cluster.mongodb.net/database\n"
                "   3. Update: railway variables set MONGODB_URI='correct-uri'\n"
                "   4. Redeploy: railway up\n"
                "\n"
                "‚ö†Ô∏è Database connection will fail with invalid URI format"
            )
        
        # Check for common URI issues
        if '@' not in uri:
            raise ValueError(
                "üö® MONGODB_URI missing authentication\n"
                "üìã URI appears incomplete - missing username@password section\n"
                "üîß Expected format: mongodb+srv://username:password@host/database"
            )
        
        return True
    
    @classmethod
    def validate_supabase_mgmt_url(cls) -> bool:
        """
        Validate SUPABASE_MGMT_URL format if configured
        
        NOTE: Supabase is OPTIONAL. The bot will function with MongoDB only.
        This validation only checks format when Supabase URL is provided.
        
        Returns:
            bool: True if URL is valid or not configured (optional)
            
        Raises:
            ValueError: If URL is provided but has invalid format
        """
        import logging
        logger = logging.getLogger(__name__)
        
        url = cls.SUPABASE_MGMT_URL
        
        # IMPORTANT: Supabase is OPTIONAL - if not configured, that's OK
        if not url:
            if cls.IS_PRODUCTION:
                logger.warning(
                    "‚ö†Ô∏è  SUPABASE_MGMT_URL not configured - enhanced user data features will be disabled\n"
                    "   The bot will function with MongoDB only (core features intact)\n"
                    "   To enable Supabase:\n"
                    "   1. Create Supabase project at https://supabase.com/\n"
                    "   2. Get connection string from Settings ‚Üí Database\n"
                    "   3. Set: railway variables set SUPABASE_MGMT_URL='postgresql://...'"
                )
            return True  # Not configured is OK - Supabase is optional
        
        # Validate URL format if provided
        if url:
            if not (url.startswith('postgresql://') or url.startswith('postgres://')):
                raise ValueError(
                    f"üö® INVALID PostgreSQL URL format\n"
                    f"üìã URL VALIDATION FAILED:\n"
                    f"   ‚Ä¢ Must start with 'postgresql://' or 'postgres://'\n"
                    f"   ‚Ä¢ Current URL starts with: '{url.split('://')[0] if '://' in url else 'INVALID'}://'\n"
                    "\n"
                    "üîß RAILWAY FIX:\n"
                    "   1. Verify complete PostgreSQL connection string\n"
                    "   2. Update: railway variables set SUPABASE_MGMT_URL='correct-url'\n"
                    "   3. Or use Railway PostgreSQL service for automatic configuration\n"
                    "\n"
                    "‚ö†Ô∏è Database connection will fail with invalid URL format"
                )
            
            # Check for authentication in URL
            if '@' not in url:
                raise ValueError(
                    "üö® PostgreSQL URL missing authentication\n"
                    "üìã URL appears incomplete - missing username:password section\n"
                    "üîß Expected format: postgresql://username:password@host:port/database"
                )
        
        return True
    
    @classmethod
    def validate_encryption_seed(cls) -> bool:
        """
        Validate ENCRYPTION_SEED with production security requirements
        
        Returns:
            bool: True if encryption seed is valid
            
        Raises:
            ValueError: If seed is missing in production or too weak
        """
        seed = cls.ENCRYPTION_SEED
        
        if cls.IS_PRODUCTION and not seed:
            # This should already be caught by the earlier check, but double-check
            raise ValueError(
                "üö® CRITICAL SECURITY ERROR: ENCRYPTION_SEED missing in production\n"
                "This error should have been caught earlier - check production detection logic"
            )
        
        if seed:
            # Validate encryption seed strength
            if len(seed) < 32:
                raise ValueError(
                    f"üö® WEAK ENCRYPTION_SEED detected\n"
                    f"üìã SECURITY VALIDATION:\n"
                    f"   ‚Ä¢ Current seed length: {len(seed)} characters\n"
                    f"   ‚Ä¢ Minimum required: 32 characters\n"
                    f"   ‚Ä¢ Recommended: 64+ characters for maximum security\n"
                    "\n"
                    "üîß RAILWAY FIX:\n"
                    "   1. Generate strong random string (32+ characters)\n"
                    "   2. Include letters, numbers, and special characters\n"
                    "   3. Set: railway variables set ENCRYPTION_SEED='your-strong-key-here'\n"
                    "   4. Redeploy: railway up\n"
                    "\n"
                    "‚ö†Ô∏è SECURITY RISK: Weak encryption compromises data protection"
                )
            
            # Check for basic entropy (not just repeated characters)
            if len(set(seed)) < 8:
                raise ValueError(
                    f"üö® LOW ENTROPY ENCRYPTION_SEED detected\n"
                    f"üìã SECURITY ISSUE:\n"
                    f"   ‚Ä¢ Seed uses only {len(set(seed))} unique characters\n"
                    f"   ‚Ä¢ Minimum recommended: 8+ unique characters\n"
                    f"   ‚Ä¢ Avoid simple patterns or repeated characters\n"
                    "\n"
                    "üîß RAILWAY FIX:\n"
                    "   1. Generate random string with mixed characters\n"
                    "   2. Example: openssl rand -base64 48\n"
                    "   3. Set: railway variables set ENCRYPTION_SEED='generated-random-string'\n"
                    "\n"
                    "‚ö†Ô∏è SECURITY RISK: Low entropy reduces encryption strength"
                )
        
        return True
    
    # ===== COMPREHENSIVE VALIDATION MASTER METHOD =====
    
    @classmethod
    def validate_all_environment_variables(cls) -> bool:
        """
        Master validation method for all critical environment variables
        This is the single entry point for comprehensive configuration validation
        
        Returns:
            bool: True if all validations pass
            
        Raises:
            ValueError: If any critical validation fails with detailed Railway-specific guidance
        """
        validation_errors = []
        
        # Run all individual validations and collect errors
        validation_methods = [
            ('TELEGRAM_BOT_TOKEN', cls.validate_telegram_bot_token),
            ('HF_TOKEN', cls.validate_hf_token), 
            ('MONGODB_URI', cls.validate_mongodb_uri),
            ('SUPABASE_MGMT_URL', cls.validate_supabase_mgmt_url),
            ('ENCRYPTION_SEED', cls.validate_encryption_seed),
            ('Railway Environment', cls.validate_railway_environment),
            ('Production Security', cls.validate_production_security)
        ]
        
        for name, method in validation_methods:
            try:
                method()
            except ValueError as e:
                validation_errors.append(f"\nüî¥ {name} VALIDATION FAILED:\n{str(e)}")
            except Exception as e:
                validation_errors.append(f"\nüî¥ {name} UNEXPECTED ERROR: {str(e)}")
        
        # If there are any validation errors, compile them into a comprehensive report
        if validation_errors:
            raise ValueError(
                "üö® CONFIGURATION VALIDATION FAILED FOR RAILWAY DEPLOYMENT\n"
                f"üìã DETECTED {len(validation_errors)} CRITICAL CONFIGURATION ISSUES:\n" + 
                "\n" + "=" * 80 + "\n".join(validation_errors) + 
                "\n" + "=" * 80 + 
                "\n\nüéØ RAILWAY DEPLOYMENT SUMMARY:\n"
                f"   ‚Ä¢ Environment: {'Railway' if cls.is_railway_environment() else 'Other Platform'}\n"
                f"   ‚Ä¢ Production Mode: {cls.IS_PRODUCTION}\n"
                f"   ‚Ä¢ Test Mode: {cls.is_test_mode()}\n"
                f"   ‚Ä¢ Validation Errors: {len(validation_errors)}\n"
                "\n"
                "üìã RAILWAY DEPLOYMENT CHECKLIST:\n"
                "   1. Fix all validation errors listed above\n"
                "   2. Set environment variables in Railway dashboard\n"
                "   3. Ensure TEST_MODE is not set or is 'false'\n"
                "   4. Verify all database connection strings\n"
                "   5. Confirm API tokens are valid and complete\n"
                "   6. Redeploy after fixing all issues\n"
                "\n"
                "üîß RAILWAY CLI COMMANDS:\n"
                "   railway variables list          # View current variables\n"
                "   railway variables set KEY=value # Set variables\n"
                "   railway up                     # Deploy with new config\n"
                "\n"
                "‚ö†Ô∏è DEPLOYMENT BLOCKED UNTIL ALL ISSUES ARE RESOLVED"
            )
        
        # All validations passed
        return True
    
    @classmethod
    def prevent_development_defaults_in_production(cls) -> bool:
        """
        Prevent development defaults from leaking into production deployment
        
        Returns:
            bool: True if no development defaults detected in production
            
        Raises:
            ValueError: If development defaults detected in production environment
        """
        if not cls.IS_PRODUCTION:
            return True  # Allow development defaults in non-production
        
        production_security_issues = []
        
        # Check 1: Ensure no development/debug modes are enabled
        debug_indicators = [
            ('DEBUG', 'Debug mode enabled'),
            ('FLASK_DEBUG', 'Flask debug mode enabled'),
            ('DJANGO_DEBUG', 'Django debug mode enabled'),
            ('DEVELOPMENT', 'Development mode flag set')
        ]
        
        for env_var, description in debug_indicators:
            value = os.getenv(env_var, '').lower()
            if value in ['true', '1', 'yes', 'on']:
                production_security_issues.append(f"{description} ({env_var}={value})")
        
        # Check 2: Ensure TEST_MODE is properly disabled (redundant check for security)
        if cls.is_test_mode():
            production_security_issues.append("TEST_MODE is enabled (security bypass)")
        
        # Check 3: Check for development-only configuration values
        development_defaults = [
            ('BOT_NAME', 'AI Assistant Pro'),  # Development default
            ('REQUEST_TIMEOUT', '300'),        # Development timeout (too high)
        ]
        
        for env_var, dev_default in development_defaults:
            current_value = os.getenv(env_var, '')
            if current_value == dev_default:
                production_security_issues.append(
                    f"Development default detected: {env_var}='{dev_default}' (should be customized for production)"
                )
        
        # Check 4: Ensure production-grade timeouts and limits
        timeout_checks = [
            ('REQUEST_TIMEOUT', int(os.getenv('REQUEST_TIMEOUT', '30')), 60, 'REQUEST_TIMEOUT too high for production'),
            ('MAX_RETRIES', int(os.getenv('MAX_RETRIES', '3')), 5, 'MAX_RETRIES too high for production'),
        ]
        
        for env_var, current_val, max_prod_val, message in timeout_checks:
            try:
                if current_val > max_prod_val:
                    production_security_issues.append(f"{message} ({env_var}={current_val}, max recommended: {max_prod_val})")
            except (ValueError, TypeError):
                production_security_issues.append(f"Invalid {env_var} value: {os.getenv(env_var)} (must be integer)")
        
        # Check 5: Warn about missing production-specific configurations
        production_recommendations = []
        
        if not cls.OWNER_ID:
            production_recommendations.append("OWNER_ID not set (admin features unavailable)")
        
        if cls.BOT_NAME == 'Hugging Face By AadityaLabs AI':  # Default name
            production_recommendations.append("BOT_NAME using default value (consider customizing)")
        
        # Report critical issues
        if production_security_issues:
            raise ValueError(
                f"üö® PRODUCTION SECURITY VIOLATIONS DETECTED\n"
                f"üìã {len(production_security_issues)} CRITICAL ISSUES FOUND:\n" +
                "\n".join([f"   ‚Ä¢ {issue}" for issue in production_security_issues]) +
                "\n\nüìã RAILWAY PRODUCTION FIX:\n"
                "   1. Review and fix all security issues above\n"
                "   2. Set production-appropriate values\n"
                "   3. Remove or set development flags to 'false'\n"
                "   4. Use production-grade timeouts and limits\n"
                "\n"
                "üîß RAILWAY COMMANDS:\n"
                "   railway variables delete DEBUG\n"
                "   railway variables set TEST_MODE=false\n"
                "   railway variables set REQUEST_TIMEOUT=30\n"
                "\n"
                "‚ö†Ô∏è SECURITY RISK: Development settings detected in production"
            )
        
        # Log production recommendations (warnings, not errors)
        if production_recommendations:
            import logging
            logger = logging.getLogger(__name__)
            logger.warning(
                f"‚ö†Ô∏è PRODUCTION RECOMMENDATIONS ({len(production_recommendations)} items):\n" +
                "\n".join([f"   ‚Ä¢ {rec}" for rec in production_recommendations])
            )
        
        return True

    @classmethod
    def get_mongodb_uri(cls) -> str | None:
        """Get MongoDB URI with fallback options"""
        return cls.MONGODB_URI
    
    
    @classmethod
    def get_supabase_mgmt_url(cls) -> str | None:
        """
        Get Supabase management database URL with validation and format fixing
        
        Returns:
            str | None: Supabase URL if configured, None otherwise
            
        NOTE: Supabase is OPTIONAL. This returns None if not configured.
        The bot will function with MongoDB only when Supabase is unavailable.
        Do NOT use DATABASE_URL as fallback - it may not be a Supabase database!
        """
        url = cls.SUPABASE_MGMT_URL  # Use explicit Supabase URL only, no fallbacks
        
        if url:
            # CRITICAL FIX: Use proper URL parsing instead of dangerous regex
            from urllib.parse import urlparse, urlunparse
            import logging
            logger = logging.getLogger(__name__)
            
            try:
                # Check for the specific malformed pattern: :@password@
                # Only attempt to fix URLs that clearly have this malformation
                if ':@' in url and url.count('@') >= 2:
                    # Find the malformed pattern manually without regex
                    if '://' in url:
                        scheme_part, rest = url.split('://', 1)
                        if ':@' in rest and '@' in rest[rest.index(':@') + 2:]:
                            # Extract components safely
                            user_part, remainder = rest.split(':@', 1)
                            password_and_host = remainder
                            # Find the second @ that separates password from host
                            second_at_index = password_and_host.index('@')
                            password = password_and_host[:second_at_index]
                            host_part = password_and_host[second_at_index + 1:]
                            
                            # Reconstruct the URL properly
                            fixed_url = f"{scheme_part}://{user_part}:{password}@{host_part}"
                            
                            # Validate the fixed URL can be parsed
                            parsed = urlparse(fixed_url)
                            if parsed.scheme and parsed.netloc:
                                logger.info(f"üîß Fixed malformed SUPABASE_MGMT_URL: password format corrected")
                                logger.debug(f"   Detected malformed pattern: :@...@")
                                logger.debug(f"   Applied safe string-based fix")
                                return fixed_url
                            else:
                                logger.warning(f"‚ö†Ô∏è Failed to validate fixed URL, using original")
                
                # For any other case, validate the URL and return as-is
                parsed = urlparse(url)
                if parsed.scheme and parsed.netloc:
                    return url
                else:
                    logger.warning(f"‚ö†Ô∏è Invalid database URL format detected")
                    return url
                    
            except Exception as e:
                # CRITICAL FIX: Always return original URL if parsing fails
                logger.error(f"‚ùå Error parsing database URL: {e}")
                logger.info("üîß Using original URL to prevent connection failure")
                return url
        
        return url
    
    @classmethod
    def get_supabase_user_base_url(cls) -> str | None:
        """Get Supabase user base database URL, defaults to management URL"""
        return cls.SUPABASE_USER_BASE_URL or cls.get_supabase_mgmt_url()
    
    @classmethod
    def has_supabase_config(cls) -> bool:
        """
        Check if Supabase configuration is available
        
        NOTE: Only checks for explicit SUPABASE_MGMT_URL, no fallbacks.
        Supabase is OPTIONAL - returns False if not configured.
        """
        return bool(cls.get_supabase_mgmt_url())
    
    @classmethod
    def has_mongodb_config(cls) -> bool:
        """Check if MongoDB configuration is available"""
        # CRITICAL FIX: Check specifically for MONGODB_URI or MONGO_URI as required by MongoDBProvider
        return bool(os.getenv('MONGODB_URI') or os.getenv('MONGO_URI'))
    
    @classmethod
    def has_hybrid_config(cls) -> bool:
        """Check if both Supabase and MongoDB configurations are available for hybrid mode"""
        return cls.has_supabase_config() and cls.has_mongodb_config()
    
    @classmethod
    def requires_hybrid_config(cls) -> bool:
        """Check if hybrid configuration is mandatory (always True in this implementation)"""
        return True
    
    @classmethod
    def has_strict_supabase_config(cls) -> bool:
        """
        Strict validation: Check if Supabase configuration is explicitly set
        
        NOTE: Supabase is OPTIONAL. This only checks for explicit SUPABASE_MGMT_URL.
        Do NOT fallback to DATABASE_URL as it may point to a non-Supabase PostgreSQL database.
        
        Returns:
            bool: True if SUPABASE_MGMT_URL is explicitly set, False otherwise
        """
        # Only check for explicit SUPABASE_MGMT_URL - no fallbacks
        return bool(os.getenv('SUPABASE_MGMT_URL'))
    
    @classmethod
    def has_strict_mongodb_config(cls) -> bool:
        """
        Strict validation: Check if MongoDB configuration matches actual provider requirements
        
        Returns:
            bool: True if MONGODB_URI or MONGO_URI is explicitly set
        """
        return bool(os.getenv('MONGODB_URI') or os.getenv('MONGO_URI'))
    
    @classmethod
    def has_strict_hybrid_config(cls) -> bool:
        """
        Strict validation: Check if both databases are configured with exact environment variables
        that the providers actually require (no fallbacks or substitutions)
        
        Returns:
            bool: True if both SUPABASE_MGMT_URL and (MONGODB_URI or MONGO_URI) are set
        """
        return cls.has_strict_supabase_config() and cls.has_strict_mongodb_config()
    
    @classmethod
    def validate_hybrid_config_early(cls) -> None:
        """
        Early validation that hard-fails with clear messaging when databases are missing
        
        Raises:
            ValueError: If required configurations are missing with detailed error messages
        """
        validation_errors = []
        
        # Check for MongoDB configuration
        if not cls.has_strict_mongodb_config():
            validation_errors.append(
                "MongoDB configuration is required for hybrid storage provider. "
                "Please set MONGODB_URI or MONGO_URI environment variable."
            )
        
        # Check for Supabase configuration (OPTIONAL - only warn, don't fail)
        if not cls.has_strict_supabase_config():
            import logging
            logger = logging.getLogger(__name__)
            logger.warning(
                "‚ö†Ô∏è  Supabase not configured - enhanced user data features will be disabled.\n"
                "   The bot will function with MongoDB only (core features intact).\n"
                "   To enable Supabase, set SUPABASE_MGMT_URL environment variable."
            )
            # NOTE: Don't add to validation_errors since Supabase is OPTIONAL
        
        # Fail fast if any configurations are missing
        if validation_errors:
            error_msg = (
                "CRITICAL HYBRID STORAGE CONFIGURATION ERRORS - EARLY VALIDATION FAILED:\n" + 
                "\n".join(f"  - {error}" for error in validation_errors) +
                "\n\nThe hybrid storage system requires BOTH databases to be properly configured. "
                "System will not start until these configuration issues are resolved."
            )
            raise ValueError(error_msg)
    
    @classmethod
    def get_preferred_storage_provider(cls) -> str:
        """
        Get preferred storage provider based on available configuration
        For hybrid architecture, returns 'hybrid' when both are available
        
        Returns:
            str: 'hybrid', 'supabase', 'mongodb', or raises error
        """
        has_supabase = cls.has_supabase_config()
        has_mongodb = cls.has_mongodb_config()
        
        if has_supabase and has_mongodb:
            return 'hybrid'  # Both databases available - use hybrid provider
        elif has_supabase:
            return 'supabase'
        elif has_mongodb:
            return 'mongodb'
        else:
            return 'hybrid'  # Default to hybrid but will fail validation
    
    @classmethod
    def is_pro_tier(cls) -> bool:
        """Check if user is on pro tier with access to premium models"""
        return cls.HF_TIER == 'pro'
    
    @classmethod
    def is_free_tier(cls) -> bool:
        """Check if user is on free tier with limited model access"""
        return cls.HF_TIER == 'free'
    
    # Enhanced AI Model Selection System Configuration
    ENHANCED_MODEL_SELECTION = os.getenv('ENHANCED_MODEL_SELECTION', 'true').lower() == 'true'
    MODEL_SELECTION_EXPLANATIONS = os.getenv('MODEL_SELECTION_EXPLANATIONS', 'true').lower() == 'true'
    CONVERSATION_TRACKING = os.getenv('CONVERSATION_TRACKING', 'true').lower() == 'true'
    PERFORMANCE_PREDICTION = os.getenv('PERFORMANCE_PREDICTION', 'true').lower() == 'true'
    DYNAMIC_FALLBACK_STRATEGIES = os.getenv('DYNAMIC_FALLBACK_STRATEGIES', 'true').lower() == 'true'
    FALLBACK_TO_TRADITIONAL = os.getenv('FALLBACK_TO_TRADITIONAL', 'true').lower() == 'true'
    
    # Real-time adaptation settings
    REAL_TIME_ADAPTATION_ENABLED = os.getenv('REAL_TIME_ADAPTATION_ENABLED', 'true').lower() == 'true'
    ADAPTATION_LEARNING_RATE = float(os.getenv('ADAPTATION_LEARNING_RATE', '0.1'))
    CONFIDENCE_THRESHOLD = float(os.getenv('CONFIDENCE_THRESHOLD', '0.7'))
    FALLBACK_CONFIDENCE_THRESHOLD = float(os.getenv('FALLBACK_CONFIDENCE_THRESHOLD', '0.3'))
    
    @classmethod
    def get_tier_appropriate_model(cls, model_name: str, fallback_model: Optional[str] = None) -> str:
        """Get tier-appropriate model with fallback for free tier"""
        # 2025 PRO-TIER ONLY MODELS (Large models that may be gated)
        pro_tier_models = {
            "Qwen/Qwen2.5-72B-Instruct",
            "Qwen/Qwen2.5-32B-Instruct",
            "Qwen/Qwen2.5-Coder-32B-Instruct",  # 2025: Large coding model
            "meta-llama/Llama-3.1-70B-Instruct",
            "meta-llama/Llama-3.1-8B-Instruct",  # May also be gated
            "Qwen/Qwen2.5-7B-Instruct",  # Potentially gated
            "Qwen/Qwen2.5-Coder-14B-Instruct",  # 2025: May be gated
        }
        
        # If on free tier and requesting a pro model, return fallback
        if cls.is_free_tier() and model_name in pro_tier_models:
            return fallback_model or cls.DEFAULT_TEXT_MODEL
        
        return model_name
    
    @classmethod
    def get_free_tier_models(cls) -> Dict[str, str]:
        """Get guaranteed free-tier accessible models (2025 optimized)"""
        return {
            'text': cls.DEFAULT_TEXT_MODEL,  # Qwen3-1.7B-Instruct
            'code': cls.DEFAULT_CODE_MODEL,  # Qwen2.5-Coder-7B-Instruct
            'efficient': cls.EFFICIENT_TEXT_MODEL,  # Qwen3-0.6B-Instruct
            'fast': cls.FAST_TEXT_MODEL,  # Qwen3-1.7B-Instruct
            'fallback': cls.FALLBACK_TEXT_MODEL,  # Qwen3-1.7B-Instruct
        }
    
    @classmethod
    def get_model_fallback_chain(cls, model_type: str) -> list:
        """CRITICAL FIX: Tier-aware fallback chains prioritizing free models first"""
        # CRITICAL FIX: Implement tier-aware fallback chains
        if cls.is_free_tier():
            # FREE TIER: Start with guaranteed accessible models
            fallback_chains = {
                # VERIFIED WORKING FREE TIER TEXT: Only verified models
                'text': [
                    cls.DEFAULT_TEXT_MODEL,            # Qwen/Qwen2.5-1.5B-Instruct (VERIFIED)
                    cls.BALANCED_TEXT_MODEL,           # microsoft/Phi-3-mini-4k-instruct (VERIFIED)
                    cls.FALLBACK_TEXT_MODEL,           # facebook/bart-large-cnn (VERIFIED)
                    cls.LEGACY_EFFICIENT_MODEL,        # microsoft/Phi-3-mini-4k-instruct (VERIFIED)
                    cls.TERTIARY_FALLBACK_MODEL        # facebook/bart-base (VERIFIED)
                ],
            
                # VERIFIED WORKING FREE TIER REASONING: Only verified models
                'reasoning': [
                    cls.REASONING_TEXT_MODEL,          # microsoft/Phi-3-mini-4k-instruct (VERIFIED)
                    cls.DEFAULT_TEXT_MODEL,            # Qwen/Qwen2.5-1.5B-Instruct (VERIFIED)
                    cls.FALLBACK_TEXT_MODEL,           # facebook/bart-large-cnn (VERIFIED)
                    cls.LEGACY_EFFICIENT_MODEL         # microsoft/Phi-3-mini-4k-instruct (VERIFIED)
                ],
                
                # VERIFIED FREE TIER MATH: Only verified math-capable models
                'math': [
                    cls.MATH_TEXT_MODEL,               # microsoft/Phi-3-mini-4k-instruct (VERIFIED MATH)
                    cls.DEFAULT_TEXT_MODEL,            # Qwen/Qwen2.5-1.5B-Instruct (VERIFIED)
                    cls.EFFICIENT_TEXT_MODEL,          # Qwen/Qwen2.5-0.5B-Instruct (VERIFIED)
                    cls.LIGHTWEIGHT_TEXT_MODEL         # microsoft/Phi-3-mini-4k-instruct (VERIFIED)
                ],
                
                # VERIFIED WORKING FREE TIER CODE: Only verified code models
                'code': [
                    cls.DEFAULT_CODE_MODEL,            # microsoft/Phi-3-mini-4k-instruct (VERIFIED CODE)
                    cls.EFFICIENT_7B_CODE_MODEL,       # Qwen/Qwen2.5-1.5B-Instruct (VERIFIED)
                    cls.FALLBACK_CODE_MODEL,           # microsoft/Phi-3-mini-4k-instruct (VERIFIED)
                    cls.LEGACY_EFFICIENT_CODE_MODEL,   # Qwen/Qwen2.5-1.5B-Instruct (VERIFIED)
                    cls.TERTIARY_CODE_FALLBACK         # facebook/bart-large-cnn (VERIFIED)
                ],
                
                # VERIFIED FREE TIER TOOL USE: Only verified tool-capable models
                'tool_use': [
                    cls.TOOL_USE_CODE_MODEL,           # microsoft/Phi-3-mini-4k-instruct (VERIFIED TOOLS)
                    cls.DEFAULT_CODE_MODEL,            # microsoft/Phi-3-mini-4k-instruct (VERIFIED)
                    cls.EFFICIENT_7B_CODE_MODEL,       # Qwen/Qwen2.5-1.5B-Instruct (VERIFIED)
                    cls.LIGHTWEIGHT_CODE_MODEL         # microsoft/Phi-3-mini-4k-instruct (VERIFIED)
                ],
                
                # VERIFIED FREE TIER EFFICIENCY: Only verified efficient models
                'efficiency': [
                    cls.EFFICIENT_TEXT_MODEL,          # Qwen/Qwen2.5-0.5B-Instruct (VERIFIED EFFICIENT)
                    cls.FAST_TEXT_MODEL,               # Qwen/Qwen2.5-0.5B-Instruct (VERIFIED FAST)
                    cls.DEFAULT_TEXT_MODEL,            # Qwen/Qwen2.5-1.5B-Instruct (VERIFIED)
                    cls.LIGHTWEIGHT_TEXT_MODEL         # microsoft/Phi-3-mini-4k-instruct (VERIFIED)
                ],
            
                # FREE TIER VISION AND NLP: Use available models with free tier priority
                'image_classification': [cls.DEFAULT_IMAGE_CLASSIFICATION_MODEL, cls.FAST_IMAGE_CLASSIFICATION_MODEL, cls.FALLBACK_IMAGE_CLASSIFICATION_MODEL],
                'image_to_text': [cls.DEFAULT_IMAGE_CAPTIONING_MODEL, cls.FAST_IMAGE_CAPTIONING_MODEL, cls.FALLBACK_IMAGE_CAPTIONING_MODEL],
                'object_detection': [cls.DEFAULT_OBJECT_DETECTION_MODEL, cls.FAST_OBJECT_DETECTION_MODEL, cls.FALLBACK_OBJECT_DETECTION_MODEL],
                'visual_question_answering': [cls.DEFAULT_VQA_MODEL, cls.FALLBACK_VQA_MODEL],
                'vision': [cls.DEFAULT_VISION_MODEL, cls.FAST_VISION_MODEL, cls.FALLBACK_VISION_MODEL],
                'gui_automation': [cls.DEFAULT_GUI_MODEL, cls.LIGHTWEIGHT_GUI_MODEL],
                
                # 2025 FREE TIER NLP: Modern models for better NLP performance
                'generative_qa': [
                    cls.DEFAULT_GENERATIVE_QA_MODEL,   # Qwen3-1.7B-Instruct (2025 SUPERIOR QA)
                    cls.DEFAULT_TEXT_MODEL,            # Qwen3-1.7B-Instruct (2025 GENERAL)
                    cls.EFFICIENT_TEXT_MODEL           # Qwen3-0.6B-Instruct (2025 EFFICIENT)
                ],
                'extractive_qa': [cls.DEFAULT_QA_MODEL, cls.ADVANCED_QA_MODEL, cls.FALLBACK_QA_MODEL],
                'qa': [cls.DEFAULT_QA_MODEL, cls.FALLBACK_QA_MODEL],
                'ner': [cls.DEFAULT_NER_MODEL, cls.ADVANCED_NER_MODEL, cls.FALLBACK_NER_MODEL],
                'ner_complex': [cls.LEGACY_NER_MODEL, cls.LEGACY_MULTILINGUAL_NER_MODEL],
                'sentiment': [cls.DEFAULT_SENTIMENT_MODEL, cls.FALLBACK_SENTIMENT_MODEL],
                'summarization': [cls.DEFAULT_SUMMARIZATION_MODEL, cls.FALLBACK_SUMMARIZATION_MODEL],
                'translation': [cls.DEFAULT_TRANSLATION_MODEL, cls.FALLBACK_TRANSLATION_MODEL],
                'classification': [cls.DEFAULT_CLASSIFICATION_MODEL, cls.FALLBACK_CLASSIFICATION_MODEL],
                'image_generation': [
                    cls.DEFAULT_IMAGE_GENERATION_MODEL,    # FLUX.1-schnell (OCT 2025: FREE, ultra-fast)
                    cls.FAST_IMAGE_GENERATION_MODEL,       # FLUX.1-schnell (OCT 2025: 1-4 steps)
                    cls.FALLBACK_IMAGE_GENERATION_MODEL    # Stable Diffusion XL (OCT 2025: reliable)
                ],
                'image_prompt': [
                    cls.DEFAULT_IMAGE_PROMPT_MODEL,    # Qwen2.5-7B-Instruct (OCT 2025: best prompts)
                    cls.COMMERCIAL_IMAGE_MODEL,        # Qwen2.5-7B-Instruct (OCT 2025: professional)
                    cls.TURBO_IMAGE_MODEL              # google/gemma-2-2b-it (OCT 2025: fast)
                ]
            }
        else:
            # PRO TIER: Full capability with 72B ‚Üí 32B ‚Üí smaller model degradation
            fallback_chains = {
                # 2025 PRO TIER TEXT: 72B ‚Üí Qwen3-8B ‚Üí Qwen3-4B ‚Üí free fallbacks
                'text': [
                    cls.ULTRA_PERFORMANCE_TEXT_MODEL,  # Qwen2.5-72B-Instruct (PRO ULTIMATE)
                    cls.HIGH_PERFORMANCE_TEXT_MODEL,   # Qwen3-8B-Instruct (2025 HIGH PERFORMANCE)
                    cls.FLAGSHIP_TEXT_MODEL,           # Qwen3-4B-Instruct (2025 FLAGSHIP)
                    cls.LEGACY_FLAGSHIP_TEXT_MODEL,    # Qwen2.5-7B-Instruct (PRO/GATED)
                    cls.DEFAULT_TEXT_MODEL,            # Qwen3-1.7B-Instruct (2025 FREE)
                    cls.EFFICIENT_TEXT_MODEL           # Qwen3-0.6B-Instruct (2025 EFFICIENT)
                ],
                'reasoning': [
                    cls.REASONING_TEXT_MODEL,          # Qwen3-4B-Instruct (2025 ENHANCED REASONING)
                    cls.ULTRA_PERFORMANCE_TEXT_MODEL,  # Qwen2.5-72B-Instruct (PRO ULTIMATE)
                    cls.ADVANCED_TEXT_MODEL,           # Qwen2.5-32B-Instruct (PRO)
                    cls.DEFAULT_TEXT_MODEL,            # Qwen3-1.7B-Instruct (2025 FREE)
                    cls.EFFICIENT_TEXT_MODEL           # Qwen3-0.6B-Instruct (2025 EFFICIENT)
                ],
                'math': [
                    cls.MATH_TEXT_MODEL,               # Qwen3-4B-Instruct (2025 EFFICIENT MATH)
                    cls.ULTRA_PERFORMANCE_TEXT_MODEL,  # Qwen2.5-72B-Instruct (PRO ULTIMATE)
                    cls.HIGH_PERFORMANCE_TEXT_MODEL,   # Qwen3-8B-Instruct (2025 HIGH PERFORMANCE)
                    cls.DEFAULT_TEXT_MODEL,            # Qwen3-1.7B-Instruct (2025 FREE)
                    cls.EFFICIENT_TEXT_MODEL           # Qwen3-0.6B-Instruct (2025 EFFICIENT)
                ],
                'code': [
                    cls.ULTRA_PERFORMANCE_CODE_MODEL,  # Qwen2.5-Coder-32B-Instruct (2025 CODING SPECIALIST)
                    cls.HIGH_PERFORMANCE_CODE_MODEL,   # Qwen2.5-Coder-14B-Instruct (2025 CODING)
                    cls.ADVANCED_CODE_MODEL,           # Qwen3-4B-Instruct (2025 EXCELLENT CODING)
                    cls.DEFAULT_CODE_MODEL,            # Qwen2.5-Coder-7B-Instruct (FREE)
                    cls.EFFICIENT_7B_CODE_MODEL,       # Qwen3-1.7B-Instruct (FREE)
                    cls.FALLBACK_CODE_MODEL            # Qwen3-1.7B-Instruct (FREE)
                ],
                'tool_use': [
                    cls.TOOL_USE_CODE_MODEL,           # Qwen3-4B-Instruct (2025 EXCELLENT TOOL INTEGRATION)
                    cls.ULTRA_PERFORMANCE_CODE_MODEL,  # Qwen2.5-Coder-32B-Instruct (2025 CODING SPECIALIST)
                    cls.HIGH_PERFORMANCE_CODE_MODEL,   # Qwen2.5-Coder-14B-Instruct (PRO)
                    cls.DEFAULT_CODE_MODEL,            # Qwen2.5-Coder-7B-Instruct (FREE)
                    cls.EFFICIENT_7B_CODE_MODEL        # Qwen3-1.7B-Instruct (FREE)
                ],
                'efficiency': [
                    cls.FAST_TEXT_MODEL,               # Start with efficient (FREE)
                    cls.EFFICIENT_TEXT_MODEL,          # FREE
                    cls.DEFAULT_TEXT_MODEL,            # FREE
                    cls.HIGH_PERFORMANCE_TEXT_MODEL    # Scale to PRO if needed
                ],
                # PRO TIER VISION: Enhanced vision capabilities with premium models
                'vision': [
                    cls.DEFAULT_VISION_MODEL,          # Best vision model for pro tier
                    cls.FAST_VISION_MODEL,             # Fast vision processing
                    cls.FALLBACK_VISION_MODEL,         # Reliable vision fallback
                    cls.DEFAULT_IMAGE_CLASSIFICATION_MODEL,  # Image classification
                    cls.DEFAULT_IMAGE_CAPTIONING_MODEL,      # Image to text
                    cls.DEFAULT_VQA_MODEL              # Visual question answering
                ]
            }
        return fallback_chains.get(model_type, [])
    
    @classmethod
    def get_model_temperature(cls, model_name: str) -> float:
        """Get optimal temperature for specific model"""
        if 'deepseek' in model_name.lower():
            return cls.DEEPSEEK_TEMPERATURE
        elif 'qwen' in model_name.lower():
            return cls.QWEN_TEMPERATURE
        elif 'starcoder' in model_name.lower() or 'coder' in model_name.lower():
            return cls.STARCODER_TEMPERATURE
        elif any(vision_term in model_name.lower() for vision_term in ['vision', 'vl', 'florence', 'minicp']):
            return cls.VISION_TEMPERATURE
        else:
            return 0.7  # Default temperature
    
    @classmethod
    def get_model_max_tokens(cls, model_name: str) -> int:
        """Get optimal max tokens for specific model"""
        if 'deepseek' in model_name.lower():
            return cls.DEEPSEEK_MAX_TOKENS
        elif 'qwen' in model_name.lower():
            return cls.QWEN_MAX_TOKENS
        elif any(code_term in model_name.lower() for code_term in ['coder', 'starcoder', 'code']):
            return cls.CODE_MAX_TOKENS
        else:
            return 1500  # Default max tokens
    
    @classmethod
    def get_validated_models(cls) -> dict:
        """Get list of models that are known to work on HF Inference API - Updated 2025"""
        # Models verified to work on HF Inference API as of 2025
        return {
            'text_models': [
                # 2025 Top Performers
                "Qwen/Qwen2.5-72B-Instruct",           # Best overall (83.1 MATH, 55.5 LiveCodeBench)
                "Qwen/Qwen2.5-32B-Instruct",           # Beats 72B in evaluations
                "meta-llama/Llama-3.1-70B-Instruct",   # 8 languages, 128K context
                "meta-llama/Llama-3.1-8B-Instruct",    # Excellent balance
                "mistralai/Mistral-7B-Instruct-v0.3",  # Most efficient 7B
                "Qwen/Qwen2.5-3B-Instruct",            # Efficiency leader
                "Qwen/Qwen2.5-1.5B-Instruct",          # Edge/mobile optimized
                # Legacy models (verified working)
                "Qwen/Qwen2.5-7B-Instruct",            # Strong fallback
                "microsoft/Phi-3-mini-4k-instruct",    # Legacy efficient
                "HuggingFaceH4/zephyr-7b-beta"         # Reliable fallback
            ],
            'image_classification': [
                "openai/clip-vit-large-patch14"
            ],
            'image_to_text': [
                "nlpconnect/vit-gpt2-image-captioning",
                "Salesforce/blip-image-captioning-base"
            ],
            'object_detection': [
                "facebook/detr-resnet-50",
                "google/owlvit-base-patch32"
            ],
            'visual_question_answering': [
                "dandelin/vilt-b32-finetuned-vqa"
            ],
            'extractive_qa': [
                "distilbert-base-cased-distilled-squad",
                "deepset/roberta-base-squad2"
            ],
            'ner': [
                "dslim/bert-base-NER",
                "Davlan/bert-base-multilingual-cased-ner-hrl"
            ],
            'sentiment_classification': [
                "cardiffnlp/twitter-roberta-base-sentiment-latest",
                "j-hartmann/emotion-english-distilroberta-base"
            ],
            'other_tasks': [
                "t5-small",
                "Helsinki-NLP/opus-mt-mul-en",
                "openai/whisper-tiny"
            ],
            # 2025 Model Performance Tiers
            'ultra_performance': [
                "Qwen/Qwen2.5-72B-Instruct"  # Best overall performance
            ],
            'high_performance': [
                "Qwen/Qwen2.5-32B-Instruct",
                "meta-llama/Llama-3.1-70B-Instruct"
            ],
            'balanced_performance': [
                "meta-llama/Llama-3.1-8B-Instruct",
                "mistralai/Mistral-7B-Instruct-v0.3",
                "Qwen/Qwen2.5-7B-Instruct"
            ],
            'efficiency_optimized': [
                "Qwen/Qwen2.5-3B-Instruct",
                "Qwen/Qwen2.5-1.5B-Instruct"
            ]
        }
    
    @classmethod 
    def _is_railway_environment(cls) -> bool:
        """
        Detect if running on Railway.com platform
        
        Returns:
            bool: True if running on Railway, False otherwise
        """
        railway_indicators = [
            'RAILWAY_ENVIRONMENT',      # Railway environment name
            'RAILWAY_STATIC_URL',       # Railway static URL  
            'RAILWAY_PROJECT_ID',       # Railway project ID
            'RAILWAY_SERVICE_NAME',     # Railway service name
            'RAILWAY_DEPLOYMENT_ID'     # Railway deployment ID
        ]
        return any(os.getenv(indicator) for indicator in railway_indicators)
    
    @classmethod
    def _get_environment_type(cls) -> str:
        """
        Detect environment type for security policy enforcement with enhanced Railway.com detection
        
        SECURITY CRITICAL: Platform indicators (Railway, Heroku, Vercel) are checked FIRST
        to prevent production bypass via generic environment variables like ENVIRONMENT=development
        """
        # SECURITY FIX: Check platform indicators FIRST before generic environment variables
        # This prevents production bypass when ENVIRONMENT=development is set on deployment platforms
        
        # Enhanced Railway.com detection - check for multiple Railway indicators
        railway_indicators = [
            'RAILWAY_ENVIRONMENT',      # Railway environment name
            'RAILWAY_STATIC_URL',       # Railway static URL
            'RAILWAY_PROJECT_ID',       # Railway project ID
            'RAILWAY_SERVICE_NAME',     # Railway service name
            'RAILWAY_DEPLOYMENT_ID'     # Railway deployment ID
        ]
        
        # If ANY Railway indicator exists, immediately return 'production' - no exceptions
        if any(os.getenv(indicator) for indicator in railway_indicators):
            return 'production'
        
        # Additional production platform indicators (also checked before generic vars)
        if os.getenv('HEROKU_APP_NAME') or os.getenv('VERCEL_ENV'):
            return 'production'
        
        # Only check generic environment variables if NO platform indicators exist
        # This prevents bypass via ENVIRONMENT=development on production platforms
        env_vars = ['ENVIRONMENT', 'NODE_ENV', 'FLASK_ENV', 'DJANGO_SETTINGS_MODULE']
        
        for var in env_vars:
            value = os.getenv(var, '').lower()
            if value in ['production', 'prod']:
                return 'production'
            elif value in ['development', 'dev', 'debug']:
                return 'development'
        
        # Default to development for safety
        return 'development'
    
    @classmethod
    def _read_env_file(cls, path: str = ".env") -> Dict[str, str]:
        """
        Read key-value pairs from .env file for development persistence
        
        Args:
            path (str): Path to the .env file (default: ".env")
            
        Returns:
            Dict[str, str]: Dictionary of environment variables from .env file
        """
        env_vars = {}
        env_file_path = path
        
        try:
            if os.path.exists(env_file_path):
                with open(env_file_path, 'r', encoding='utf-8') as f:
                    for line_num, line in enumerate(f, 1):
                        line = line.strip()
                        
                        # Skip empty lines and comments
                        if not line or line.startswith('#'):
                            continue
                        
                        # Parse KEY=VALUE format
                        if '=' in line:
                            key, value = line.split('=', 1)
                            key = key.strip()
                            value = value.strip()
                            
                            # Remove quotes if present
                            if (value.startswith('"') and value.endswith('"')) or \
                               (value.startswith("'") and value.endswith("'")):
                                value = value[1:-1]
                            
                            env_vars[key] = value
        except Exception as e:
            import logging
            logger = logging.getLogger(__name__)
            logger.warning(f"Warning: Could not read .env file: {e}")
        
        return env_vars
    
    @classmethod
    def _write_to_env_file(cls, upserts: Dict[str, str], path: str = ".env") -> None:
        """
        Write or update key-value pairs in .env file for development persistence
        SECURITY: Only works in development environment, never in production/Railway
        
        Args:
            upserts (Dict[str, str]): Dictionary of environment variables to write/update
            path (str): Path to the .env file (default: ".env")
            
        Raises:
            ValueError: If called in production environment for security
        """
        import logging
        logger = logging.getLogger(__name__)
        
        # SECURITY: Block writes in production/Railway environment
        if cls._get_environment_type() == 'production':
            raise ValueError(
                "SECURITY VIOLATION: _write_to_env_file() is blocked in production/Railway environment. "
                "Environment variables must be set through the deployment platform's environment configuration."
            )
        
        env_file_path = path
        
        try:
            # Read existing content
            existing_lines = []
            
            if os.path.exists(env_file_path):
                with open(env_file_path, 'r', encoding='utf-8') as f:
                    existing_lines = f.readlines()
            
            # Track which keys we've already processed
            updated_lines = []
            keys_found = set()
            
            # Process existing lines, updating any matching keys
            for line in existing_lines:
                stripped_line = line.strip()
                updated_line = False
                
                for key in upserts:
                    if stripped_line.startswith(f'{key}='):
                        # Update existing key
                        updated_lines.append(f'{key}={upserts[key]}\n')
                        keys_found.add(key)
                        updated_line = True
                        break
                
                if not updated_line:
                    updated_lines.append(line)
            
            # Add new keys that weren't found
            new_keys = set(upserts.keys()) - keys_found
            if new_keys:
                # Add a comment if this is the first entry
                if not updated_lines:
                    updated_lines.append('# Auto-generated environment variables for development\n')
                
                # Add each new key
                for key in new_keys:
                    updated_lines.append(f'{key}={upserts[key]}\n')
            
            # Write back to file
            with open(env_file_path, 'w', encoding='utf-8') as f:
                f.writelines(updated_lines)
            
            logger.info(f"‚úÖ Persisted {len(upserts)} environment variable(s) to .env file for development session")
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Could not persist environment variables to .env file: {e}")
            logger.warning("   Environment variables will not persist across restarts")
    
    @classmethod
    def has_hf_token(cls) -> bool:
        """Check if HF_TOKEN is available for AI functionality using runtime canonicalization"""
        token = cls.get_hf_token()
        return bool(token and len(token.strip()) > 0)
    
    @classmethod
    def get_hf_token(cls) -> str | None:
        """
        Get HF_TOKEN with runtime canonicalization and validation
        
        Returns the first non-empty token from:
        1. HF_TOKEN
        2. HUGGINGFACE_API_KEY  
        3. HUGGING_FACE_TOKEN
        
        This ensures tokens are correctly resolved even with timing issues.
        """
        import os
        import logging
        
        # Runtime canonicalization - check all possible token sources
        canonical_token = (
            os.getenv('HF_TOKEN') or 
            os.getenv('HUGGINGFACE_API_KEY') or 
            os.getenv('HUGGING_FACE_TOKEN')
        )
        
        if canonical_token and canonical_token.strip():
            token = canonical_token.strip()
            
            # Log token presence safely (without revealing value)
            logger = logging.getLogger(__name__)
            
            # Determine which env var provided the token for logging
            token_source = 'HF_TOKEN' if os.getenv('HF_TOKEN') else (
                'HUGGINGFACE_API_KEY' if os.getenv('HUGGINGFACE_API_KEY') else 'HUGGING_FACE_TOKEN'
            )
            
            # Safe logging - only indicates presence and source, never the actual token
            logger.debug(f"‚úÖ HF token successfully retrieved from {token_source} (length: {len(token)})")
            
            return token
            
        return None
    
    @classmethod
    def _validate_hf_token_format(cls, token: str) -> tuple[bool, str]:
        """
        Validate HF_TOKEN format and basic requirements (internal helper)
        
        Args:
            token (str): The token to validate
            
        Returns:
            tuple[bool, str]: (is_valid, error_message)
        """
        if not token or not token.strip():
            return False, "HF_TOKEN is empty or None"
        
        token = token.strip()
        
        # Check basic token format (HF tokens typically start with 'hf_')
        if not token.startswith('hf_'):
            return False, "HF_TOKEN must start with 'hf_' (get it from https://huggingface.co/settings/tokens)"
        
        # Check minimum length (HF tokens are typically 37+ characters)
        if len(token) < 20:
            return False, "HF_TOKEN appears to be too short (should be 37+ characters)"
        
        # Check for valid characters (alphanumeric + underscore + hyphen)
        import re
        if not re.match(r'^hf_[A-Za-z0-9_-]+$', token):
            return False, "HF_TOKEN contains invalid characters (should only contain letters, numbers, underscores, and hyphens)"
        
        return True, "Token format appears valid"
    
    @classmethod
    def get_environment_type(cls) -> str:
        """Get current environment type (development or production)"""
        return cls._get_environment_type()
    
    @classmethod
    def is_ai_functionality_available(cls) -> tuple[bool, str]:
        """
        Check if AI functionality is available based on environment and token
        
        Returns:
            tuple[bool, str]: (is_available, status_message)
        """
        env_type = cls.get_environment_type()
        has_token = cls.has_hf_token()
        
        if env_type == 'production':
            if not has_token:
                return False, "‚ùå AI functionality disabled: HF_TOKEN required in production"
            else:
                # Add null check before calling validate_hf_token
                if cls.HF_TOKEN is not None:
                    is_valid, error_msg = cls._validate_hf_token_format(cls.HF_TOKEN)
                    if not is_valid:
                        return False, f"‚ùå AI functionality disabled: {error_msg}"
                    return True, "‚úÖ AI functionality available"
                else:
                    return False, "‚ùå AI functionality disabled: HF_TOKEN is None"
        else:  # development
            if not has_token:
                return False, "‚ö†Ô∏è AI functionality disabled: HF_TOKEN not configured (optional in development)"
            else:
                # Add null check before calling validate_hf_token
                if cls.HF_TOKEN is not None:
                    is_valid, error_msg = cls._validate_hf_token_format(cls.HF_TOKEN)
                    if not is_valid:
                        # In development, provide soft warning instead of hard block
                        return True, f"‚ö†Ô∏è AI functionality enabled with validation warning: {error_msg}"
                    return True, "‚úÖ AI functionality available"
                else:
                    return False, "‚ö†Ô∏è AI functionality disabled: HF_TOKEN is None"
    
    @classmethod
    def get_ai_setup_instructions(cls) -> str:
        """Get user-friendly instructions for setting up AI functionality"""
        env_type = cls.get_environment_type()
        
        instructions = """
ü§ñ **UNLOCK UNLIMITED AI POWER!**

To access all AI features (image analysis, code generation, advanced reasoning), you need a FREE Hugging Face token:

**üìã STEP-BY-STEP SETUP INSTRUCTIONS:**

**Step 1:** üåê Go to: https://huggingface.co/settings/tokens
**Step 2:** üîë Click "New token" ‚Üí Choose "Read" access  
**Step 3:** üìã Copy your token (starts with 'hf_...')
**Step 4:** üíæ Set it as environment variable: `HF_TOKEN=your_token_here`

**üí∞ 100% FREE FOREVER:**
‚Ä¢ No subscription fees (unlike ChatGPT Plus $20/month)
‚Ä¢ No usage limits (unlike Claude Pro)
‚Ä¢ Access to 50+ cutting-edge models
‚Ä¢ Better than GPT-4 for many tasks!

**üöÄ INSTANT BENEFITS:**
‚Ä¢ üìä Advanced document analysis
‚Ä¢ üñºÔ∏è Intelligent image processing  
‚Ä¢ üíª Superior code generation
‚Ä¢ üß† Mathematical reasoning
‚Ä¢ üé® Creative content generation

**Need help?** Use the `/setup` command for guided configuration!
        """
        
        if env_type == 'production':
            instructions += "\n\n‚ö° **Production Environment**: HF_TOKEN is required for AI functionality."
        else:
            instructions += "\n\nüõ†Ô∏è **Development Environment**: HF_TOKEN is optional but recommended for full functionality."
        
        return instructions.strip()
    
    @classmethod
    def ensure_encryption_seed(cls) -> str:
        """
        Ensure ENCRYPTION_SEED is available, auto-generating in development if needed
        Now includes persistence to .env file in development mode to prevent data loss
        
        Returns:
            str: The encryption seed value
            
        Raises:
            ValueError: If seed cannot be obtained in production
        """
        import logging
        logger = logging.getLogger(__name__)
        
        # Check if we already have a seed in memory
        if cls.ENCRYPTION_SEED and len(cls.ENCRYPTION_SEED) >= 32:
            return cls.ENCRYPTION_SEED
        
        # Detect environment type
        environment = cls._get_environment_type()
        is_production = environment == 'production'
        
        if not cls.ENCRYPTION_SEED:
            if is_production:
                raise ValueError(
                    "CRITICAL SECURITY ERROR: ENCRYPTION_SEED environment variable is REQUIRED for production deployment. "
                    "Auto-generation is not allowed in production to ensure deterministic, persistent encryption."
                )
            else:
                # Development mode: Check .env file first, then auto-generate if needed
                env_vars = cls._read_env_file()
                existing_seed = env_vars.get('ENCRYPTION_SEED')
                
                if existing_seed and len(existing_seed) >= 32:
                    # Use existing persistent seed from .env file
                    cls.ENCRYPTION_SEED = existing_seed
                    os.environ['ENCRYPTION_SEED'] = existing_seed
                    logger.info("üîê Using persistent encryption seed from .env file (value redacted for security)")
                    logger.info("‚úÖ Encrypted data will remain accessible across restarts")
                    return existing_seed
                else:
                    # Auto-generate new seed and persist it
                    import secrets
                    import base64
                    generated_seed = base64.b64encode(secrets.token_bytes(32)).decode()
                    
                    # Persist to .env file for future restarts
                    cls._write_to_env_file({'ENCRYPTION_SEED': generated_seed})
                    
                    # Set in memory and environment
                    cls.ENCRYPTION_SEED = generated_seed
                    os.environ['ENCRYPTION_SEED'] = generated_seed
                    
                    logger.warning("üîê Auto-generated NEW encryption seed for development (seed value redacted for security)")
                    logger.warning("‚úÖ Seed persisted to .env file - encrypted data will survive restarts")
                    logger.warning("‚ö†Ô∏è  For production deployment, set ENCRYPTION_SEED environment variable to a secure 32+ character value")
                    return generated_seed
        elif len(cls.ENCRYPTION_SEED) < 32:
            raise ValueError("ENCRYPTION_SEED must be at least 32 characters for security")
        
        return cls.ENCRYPTION_SEED
    
    @classmethod
    def get_encryption_seed(cls) -> str:
        """
        Get the encryption seed, ensuring it's available
        
        This is an alias to ensure_encryption_seed() for API clarity and backward compatibility.
        Recommended by architect for use with initialize_crypto().
        
        Returns:
            str: The encryption seed value
            
        Raises:
            ValueError: If seed cannot be obtained in production
        """
        return cls.ensure_encryption_seed()
    
    @classmethod
    def validate_config(cls):
        """Validate critical security configuration requirements with fail-fast approach"""
        import logging
        import re
        logger = logging.getLogger(__name__)
        
        validation_errors = []
        environment = cls._get_environment_type()
        is_production = environment == 'production'
        
        logger.info(f"üåê Environment detected: {environment}")
        
        # CRITICAL: Always require Telegram bot token
        if not cls.TELEGRAM_BOT_TOKEN:
            validation_errors.append("TELEGRAM_BOT_TOKEN environment variable is required")
        elif not re.match(r'^\d+:[a-zA-Z0-9_-]{35,}$', cls.TELEGRAM_BOT_TOKEN):
            # Validate proper bot token format (bot_id:token_hash) - flexible for development
            logger.warning("‚ö†Ô∏è  TELEGRAM_BOT_TOKEN format may be invalid (expected: digits:hash format)")
            logger.warning("   For production, ensure you have a valid bot token from @BotFather")
        
        # SECURITY CRITICAL: ENCRYPTION_SEED handling with production fail-fast
        # Use the new ensure_encryption_seed method to handle auto-generation
        try:
            seed = cls.ensure_encryption_seed()
            if is_production:
                logger.info("‚úÖ ENCRYPTION_SEED provided for production (value redacted for security)")
            else:
                logger.info("‚úÖ ENCRYPTION_SEED available for development (value redacted for security)")
        except ValueError as e:
            validation_errors.append(str(e))
        
        # Database validation - REQUIRE BOTH MongoDB AND Supabase for hybrid configuration
        mongo_uri = cls.get_mongodb_uri()
        supabase_url = cls.get_supabase_mgmt_url()
        
        if not mongo_uri:
            validation_errors.append(
                "MONGODB_URI is required for hybrid storage. "
                "Set MONGODB_URI environment variable for API keys, telegram IDs, and developer database."
            )
        
        if not supabase_url:
            validation_errors.append(
                "SUPABASE_MGMT_URL is required for hybrid storage. "
                "Set SUPABASE_MGMT_URL environment variable for user data storage."
            )
        
        # Enhanced MongoDB URI validation with TLS enforcement
        if mongo_uri:
            if not (mongo_uri.startswith('mongodb://') or mongo_uri.startswith('mongodb+srv://')):
                validation_errors.append("MONGODB_URI must be a valid MongoDB connection string (mongodb:// or mongodb+srv://)")
            elif 'mongodb+srv://' in mongo_uri:
                # Production TLS enforcement for MongoDB Atlas
                if 'tls=true' not in mongo_uri.lower() and 'ssl=true' not in mongo_uri.lower():
                    logger.warning("‚ö†Ô∏è  SECURITY: Consider enabling TLS for MongoDB Atlas in production")
            
            # Validate URI contains authentication for production
            if '@' not in mongo_uri:
                logger.warning("‚ö†Ô∏è  SECURITY: MongoDB URI should include authentication for production")
            
            logger.info("‚úÖ MongoDB configuration detected and validated")
        
        # Enhanced Supabase URL validation  
        if supabase_url:
            if not (supabase_url.startswith('postgresql://') or supabase_url.startswith('postgres://')):
                validation_errors.append("SUPABASE_MGMT_URL must be a valid PostgreSQL connection string (postgresql:// or postgres://)")
            elif 'sslmode=' not in supabase_url.lower():
                logger.warning("‚ö†Ô∏è  SECURITY: Consider specifying sslmode=require for Supabase in production")
            
            logger.info("‚úÖ Supabase configuration detected and validated")
        
        # Note: Hugging Face API keys are provided individually by users
        # No global HF_TOKEN is required - each user provides their own API key
        logger.info("‚úÖ Individual user API key system configured (no global HF_TOKEN needed)")
        
        # Fail fast if critical errors found
        if validation_errors:
            error_msg = "CRITICAL CONFIGURATION ERRORS:\n" + "\n".join(f"  - {error}" for error in validation_errors)
            logger.error(error_msg)
            raise ValueError(error_msg)
        
        # Production health checks
        if is_production:
            cls._perform_production_health_checks(logger)
        
        # Log security status
        preferred_provider = cls.get_preferred_storage_provider()
        logger.info(f"üéØ Preferred storage provider: {preferred_provider}")
        logger.info(f"üîê Encryption seed: {'‚úÖ PROVIDED' if cls.ENCRYPTION_SEED else '‚ùå MISSING'}")
        logger.info(f"üè≠ Environment: {environment}")
        logger.info("‚úÖ All critical security configurations validated successfully")
        return True
    
    @classmethod
    def _perform_production_health_checks(cls, logger):
        """Perform additional health checks for production deployment"""
        logger.info("üîí Performing production security health checks...")
        
        # Check encryption seed quality
        if cls.ENCRYPTION_SEED:
            # Validate seed entropy (should be base64 encoded random bytes)
            try:
                import base64
                decoded = base64.b64decode(cls.ENCRYPTION_SEED)
                if len(decoded) >= 32:
                    logger.info("‚úÖ Encryption seed has sufficient entropy for production")
                else:
                    logger.warning("‚ö†Ô∏è  Encryption seed may have insufficient entropy for production")
            except Exception:
                logger.warning("‚ö†Ô∏è  Encryption seed format warning: consider using base64-encoded random bytes")
        
        # Check database security
        mongo_uri = cls.get_mongodb_uri()
        supabase_url = cls.get_supabase_mgmt_url()
        
        if mongo_uri:
            # MongoDB security checks
            if 'mongodb+srv://' in mongo_uri or 'tls=true' in mongo_uri.lower() or 'ssl=true' in mongo_uri.lower():
                logger.info("‚úÖ MongoDB TLS encryption detected")
            else:
                logger.warning("‚ö†Ô∏è  MongoDB connection may not use TLS encryption in production")
        
        if supabase_url:
            # Supabase security checks
            if 'sslmode=' in supabase_url.lower():
                logger.info("‚úÖ Supabase SSL mode configured")
            else:
                logger.info("‚ÑπÔ∏è  Consider specifying sslmode=require for Supabase production deployment")
        
        logger.info("‚úÖ Production health checks completed")

# Initialize TEST_MODE after class creation
Config.TEST_MODE = Config._validate_test_mode_security()